


import sys

sys.path.insert(1, "/home/vinicius/storage1/projects/vanderbilt")


import os
from functools import partial

import emd
import matplotlib.pyplot as plt
import numpy as np
import PyEMD
import skimage as ski
import umap
import xarray as xr
from frites.utils import parallel_func
from mne.time_frequency import psd_array_multitaper, psd_array_welch
from scipy.optimize import curve_fit
from skimage.segmentation import watershed
from tqdm import tqdm

from VUDA.emd import emd_vec
from VUDA.io.loadbinary import LoadBinary





date = "10-20-2022"
monkey = "FN"
channel = 21


date = "2021-09-24"
monkey = "WI"
channel = 61


filepath = os.path.expanduser(f"~/funcog/HoffmanData/{monkey}/{date}/aHPC_B_cnct.nc")


data = xr.load_dataarray(filepath)


# Get zero timestamp
t_init = data.times.data[0]
# Get final timestamp
t_end = data.times.data[0]
# End of treehouse
t_th_end = float(data.attrs["TH_end"].split(", ")[1])
# Beggining of sleep
# t_sleep_init = float(data.attrs["Sleep_start"].split(", ")[1])


data = data.sel(times=slice(t_init, t_th_end))


times = data.times.values


plt.figure(figsize=(20, 4))
ax = plt.subplot(111)
data.sel(channels=channel).plot()
[ax.spines[key].set_visible(False) for key in ["top", "right"]];





def standardize_imf_per_block(IMFs):
    """
    Standardizes the number of intrinsic mode functions (IMFs) per block in an xarray Dataset.
    It sums slower IMFs in case a given block has more IMFs, thant the block that has the least
    number of IMFs.

    Parameters:
    - IMFs (xarray.Dataset): Input Dataset containing IMFs with dimensions ('blocks', 'IMFs', ...).

    Returns:
    - xarray.Dataset: Output Dataset with standardized IMFs per block.

    The function standardizes the number of IMFs per block by either summing the first
    (10 - n_imfs_min + 1) IMFs or keeping the original IMFs if the number is already less
    than or equal to n_imfs_min.

    Note:
    - The function assumes that the input Dataset has dimensions ('blocks', 'IMFs', ...).
    - The result is a new Dataset with standardized IMFs per block.
    """

    assert isinstance(IMFs, xr.DataArray)
    np.testing.assert_array_equal(IMFs.dims, ("blocks", "IMFs", "times"))

    n_imfs_min = IMFs.n_imfs_per_block.min()
    attrs = IMFs.attrs

    reduced = []

    for i in range(IMFs.sizes["blocks"]):

        temp = IMFs[i].dropna("IMFs").drop_vars("IMFs")
        n_imfs = temp.shape[0]

        if n_imfs > n_imfs_min:

            reduced += [
                xr.concat(
                    (
                        temp[0 : n_imfs - n_imfs_min + 1].sum("IMFs", keepdims=True),
                        temp[n_imfs - n_imfs_min + 1 :],
                    ),
                    "IMFs",
                )
            ]

        else:
            reduced += [temp]

    IMFs = xr.concat(reduced, "blocks")
    IMFs.attrs = attrs

    return IMFs


IMFs_single = emd_vec(
    data.sel(channels=channel).values,
    times,
    method="eemd",
    max_imfs=None,
    block_size=200,
    nensembles=5,
    use_min_block_size=True,
    remove_fastest_imf=True,
    n_jobs=20,
    imf_opts={"stop_method": "fixed", "max_iters": 5},
)


IMFs_single = standardize_imf_per_block(IMFs_single)


imf = IMFs_single[0].data

z_imf = (imf - imf.mean(1)[:, None]) / imf.std(1)[:, None]


fig, axd = plt.subplot_mosaic(
    [["A", "A", "C", "C"], ["B", "B", "C", "C"], ["B", "B", "C", "C"]],
    layout="constrained",
    figsize=(10, 5),
    dpi=600,
)

# Plot the signal snippet
plt.sca(axd["A"])
plt.plot(data.sel(channels=33).data)
plt.xlim(0, 5000)
plt.xlabel("Time [a.u]")
plt.ylabel(r"LFP [$\mu$V]")
[axd["A"].spines[key].set_visible(False) for key in ["top", "right"]]

# Plot the signal IMFs
plt.sca(axd["B"])
colors = []
for i in range(imf.shape[0]):
    p_ = plt.plot(z_imf[i] + 14.5 * (z_imf.shape[0] - i))
    colors += [p_[0].get_color()]
    plt.text(
        -490,
        14.5 * (z_imf.shape[0] - i),
        f"IMF {i + 1}",
        color=colors[-1],
        rotation=20,
    )
    plt.axis("off")


psd, f = psd_array_multitaper(
    imf,
    fmin=0,
    fmax=300,
    sfreq=1000,
    bandwidth=4,
    verbose=False,
    n_jobs=10,
)

plt.sca(axd["C"])
for i in range(len(psd)):
    plt.semilogx(f, psd[i] / psd[i].sum())
plt.xlabel("frequency [Hz]")
plt.ylabel("Norm. PSD")
[axd["C"].spines[key].set_visible(False) for key in ["top", "right"]];





IMFs = IMFs_single.stack(samples=("blocks", "IMFs")).T.dropna("samples").data


SXX, f = psd_array_multitaper(
    IMFs,
    fmin=0,
    fmax=300,
    sfreq=1000,
    verbose=True,
    bandwidth=4,
    n_jobs=20,
)

SXX_norm = SXX / SXX.mean(1)[:, None]


plt.figure(figsize=(8, 4))
ax = plt.subplot(111)
plt.semilogx(f, SXX_norm.T)
plt.xlabel("frequency [Hz]")
plt.ylabel("Norm. PSD")
[ax.spines[key].set_visible(False) for key in ["top", "right"]];


def curvature(x):

    div = np.gradient(x)
    lap = np.gradient(x)

    K = np.abs(lap) / ((1 + div**2) ** 1.5)

    return K


reducer = umap.UMAP(
    n_jobs=20,
    min_dist=0.5,
    n_neighbors=5,
)
embedding = reducer.fit_transform(SXX_norm)


from sklearn.cluster import DBSCAN, OPTICS, KMeans
from sklearn.neighbors import NearestNeighbors

#  clustering = KMeans(n_clusters=6, init="k-means++", n_init="auto").fit(SXX_norm)

knn = NearestNeighbors(n_neighbors=20, n_jobs=20, metric="euclidean")
knn_fit = knn.fit(SXX_norm)

distances, indices = knn_fit.kneighbors(SXX_norm)
distances = np.sort(distances, axis=0)
distances = distances[:, 1]

eps = 4 * distances[np.argmax(curvature(distances))]

clustering = DBSCAN(eps=eps, min_samples=20).fit(SXX_norm)


clustering.labels_


plt.scatter(embedding[:, 0], embedding[:, 1], s=0.5, c=clustering.labels_, cmap="tab10")


labels = clustering.labels_.copy()


from sklearn.metrics import pairwise_distances_argmin_min

if -1 in np.unique(labels):

    labels = clustering.labels_.copy()

    # Identify noise points
    noise_points = SXX_norm[labels == -1]

    # Find the closest cluster for each noise point
    closest_cluster_indices = pairwise_distances_argmin_min(
        noise_points, SXX_norm[labels != -1]
    )[0]
    closest_cluster_labels = labels[closest_cluster_indices]

    # Assign noise points to the closest cluster
    labels[labels == -1] = closest_cluster_labels

    clustering.labels_ = labels


unique_labels = np.unique(clustering.labels_)
n_cluster = len(unique_labels)


def average_for_cluster(data=None, cluster_labels=None, label=None):
    return data[cluster_labels == label].mean(0)


partial_average_for_cluster = partial(
    average_for_cluster, data=SXX_norm, cluster_labels=clustering.labels_
)

out = np.stack([partial_average_for_cluster(label=label) for label in unique_labels])

order = np.argsort(out.argmax(axis=1))


unique_labels[order]


cluster_labels = clustering.labels_.reshape(-1, IMFs_single.shape[1])


plt.figure(figsize=(10, 5))
for pos, i in enumerate(unique_labels[order]):
    plt.subplot(3, 3, pos + 1)
    plt.semilogx(f, SXX_norm[clustering.labels_ == i].T, lw=0.1, color="b")
    plt.semilogx(f, SXX_norm[clustering.labels_ == i].mean(0), lw=2, color="k")
    plt.xlabel("Frequency [Hz]")
    plt.ylabel("Norm. POWER")
plt.tight_layout()





n_blocks = IMFs_single.shape[0]


composite = []

for i in range(n_blocks):
    labels = cluster_labels[i]
    composite += [
        IMFs_single[i]
        .dropna("IMFs")
        .assign_coords({"IMFs": labels})
        .groupby("IMFs")
        .sum("IMFs")
    ]

composite = xr.concat(composite, "blocks")


SXX_composite, f = psd_array_multitaper(
    composite,
    fmin=0,
    fmax=300,
    sfreq=1000,
    verbose=True,
    bandwidth=4,
    n_jobs=20,
)


plt.figure(figsize=(10, 5))
for pos, i in enumerate(unique_labels[order]):
    plt.subplot(3, 3, pos + 1)
    plt.semilogx(f, SXX_composite[:, i, :].T, c="b", lw=0.1)
    plt.semilogx(f, np.nanmean(SXX_composite[:, i, :], 0), c="r", lw=3)
plt.tight_layout()





date = "2021-10-06"
monkey = "WI"
max_imfs = None
method = "eemd"


IMFpath = os.path.expanduser(
    f"~/funcog/HoffmanData/{monkey}/{date}/IMFs_task_method_{method}_max_imfs_{max_imfs}.nc"
)


# Path in which to save figures
figures_path = f"figures/{monkey}/{date}"

if not os.path.isdir(figures_path):
    os.makedirs(figures_path)


IMFs_dataset = xr.open_dataset(IMFpath)


channels = list(IMFs_dataset.keys())


f_mt = partial(
    psd_array_multitaper,
    fmin=0,
    fmax=300,
    sfreq=1000,
    verbose=True,
    bandwidth=4,
    n_jobs=20,
)


def reassign_noise_points(data, labels):
    """
    Reassigns noise points in the clustering labels to the closest cluster.

    Parameters:
    - data (numpy.ndarray): Input data points.
    - labels (numpy.ndarray): Cluster labels assigned to each data point.

    Returns:
    - numpy.ndarray: Updated cluster labels with noise points reassigned to the closest cluster.

    If there are noise points represented by the label -1 in the input labels, this function
    identifies those points, finds the closest cluster for each noise point, and reassigns
    them to the cluster with the most occurrences among the closest clusters.
    """
    # Set noise points to the closest cluster
    if -1 in np.unique(labels):

        # Identify noise points
        noise_points = data[labels == -1]

        # Find the closest cluster for each noise point
        closest_cluster_indices = pairwise_distances_argmin_min(
            noise_points, data[labels != -1]
        )[0]
        closest_cluster_labels = labels[closest_cluster_indices]

        # Assign noise points to the closest cluster
        labels[labels == -1] = closest_cluster_labels

        return labels

    return labels


def curvature(x):

    div = np.gradient(x)
    lap = np.gradient(x)

    K = np.abs(lap) / ((1 + div**2) ** 1.5)

    return K


reducer = umap.UMAP(
    n_jobs=20,
    min_dist=0.5,
    n_neighbors=20,
)

for channel in channels[8:]:
    #################################################################
    # Get IMFs for channel
    #################################################################
    IMFs_single = IMFs_dataset[channel].load().dropna("IMFs")

    IMFs = IMFs_single.stack(samples=("blocks", "IMFs")).T
    #################################################################
    # Compute power-spectrum for IMFs
    #################################################################
    SXX, f = f_mt(IMFs)

    SXX_norm = SXX / SXX.mean(1)[:, None]

    #################################################################
    # Embed power-spectra
    #################################################################

    embedding = reducer.fit_transform(SXX_norm)

    knn = NearestNeighbors(n_neighbors=20, n_jobs=20, metric="euclidean")
    knn_fit = knn.fit(SXX_norm)

    distances, indices = knn_fit.kneighbors(SXX_norm)
    distances = np.sort(distances, axis=0)
    distances = distances[:, 1]

    eps = distances[np.argmax(distances)]
    # Cluster power-spectra
    clustering = DBSCAN(eps=eps, min_samples=20).fit(SXX_norm)
    clustering.labels_ = reassign_noise_points(SXX_norm, clustering.labels_)
    unique_labels = np.unique(clustering.labels_)
    n_cluster = len(unique_labels)

    #################################################################
    # Order labels by power peak
    #################################################################
    partial_average_for_cluster = partial(
        average_for_cluster, data=SXX_norm, cluster_labels=clustering.labels_
    )

    out = np.stack(
        [partial_average_for_cluster(label=label) for label in unique_labels]
    )

    order = np.argsort(out.argmax(axis=1))

    #################################################################
    # Plot embedding colored by cluster labels
    #################################################################
    plt.figure(figsize=(8, 8))
    ax = plt.subplot(111)
    plt.scatter(
        embedding[:, 0], embedding[:, 1], s=0.5, c=clustering.labels_, cmap="tab10"
    )
    plt.title(f"embedding IMFs - monkey {monkey} - date {date} - {channel}")
    plt.axis("off")
    plt.savefig(os.path.join(figures_path, f"embedding_imfs_{channel}.png"))
    plt.close()

    #################################################################
    # Plot spectra for each cluster
    #################################################################
    plt.figure(figsize=(10, 5))
    for pos, i in enumerate(unique_labels[order]):
        ax = plt.subplot(3, 3, pos + 1)
        plt.semilogx(f, SXX_norm[clustering.labels_ == i].T, lw=0.1, color="b")
        plt.semilogx(f, SXX_norm[clustering.labels_ == i].mean(0), lw=2, color="k")
        plt.xlabel("Frequency [Hz]")
        plt.ylabel("Norm. POWER")
        [ax.spines[key].set_visible(False) for key in ["top", "right"]]
    plt.tight_layout()
    plt.savefig(os.path.join(figures_path, f"clustered_imf_spectra_{channel}.png"))
    plt.close()

    #################################################################
    # Generate composite signals
    #################################################################
    n_blocks = IMFs_single.shape[0]
    cluster_labels = clustering.labels_.reshape(-1, IMFs_single.shape[1])

    composite = []

    for i in range(n_blocks):
        labels = cluster_labels[i]
        composite += [
            IMFs_single[i]
            .dropna("IMFs")
            .assign_coords({"IMFs": labels})
            .groupby("IMFs")
            .sum("IMFs")
        ]

    composite = xr.concat(composite, "blocks")

    #################################################################
    # Compute power-spectrum for composite signals
    #################################################################
    SXX_composite, f = f_mt(composite)

    plt.figure(figsize=(10, 5))
    for pos, i in enumerate(unique_labels[order]):
        ax = plt.subplot(3, 3, pos + 1)
        plt.semilogx(f, SXX_composite[:, i, :].T, c="b", lw=0.1)
        plt.semilogx(f, np.nanmean(SXX_composite[:, i, :], 0), c="r", lw=3)
        plt.xlabel("Frequency [Hz]")
        plt.ylabel("Norm. POWER")
        [ax.spines[key].set_visible(False) for key in ["top", "right"]]
    plt.tight_layout()
    plt.savefig(
        os.path.join(figures_path, f"clustered_composites_spectra_{channel}.png")
    )
    plt.close()





ROOT = os.path.expanduser(f"~/funcog/HoffmanData/{monkey}/{date}")
composite_path = "ps_composite_signals_task_method_eemd_max_imfs_None.nc"

filepath = os.path.join(ROOT, composite_path)


import jax
import jax.numpy as jnp
import scipy
import seaborn as sns


def to_bin_freq(freqs, peaks):

    n_blocks, n_peaks = peaks.shape
    n_freqs = freqs.shape[0]

    def _for_peak(carry, peak):

        vec = jnp.zeros(n_freqs, dtype=int)
        indexes = jnp.stack(
            [jnp.argmin(jnp.abs(freqs - peak[i])) for i in range(n_peaks)]
        )
        vec = vec.at[indexes].set(1)
        return carry, vec

    _, vec = jax.lax.scan(_for_peak, None, peaks)

    return np.asarray(vec)


composites = xr.load_dataset(filepath)

channels = list(composites.keys())


data = composites["channel33"].dropna("IMFs")


data.shape


freqs = data.freqs.data


kernel = np.hanning(50)


peaks = []

fg = fooof.FOOOFGroup(max_n_peaks=1, verbose=False)

for channel in tqdm(channels):
    data = composites[channel].dropna("IMFs")
    data_sm = xr.DataArray(
        scipy.signal.fftconvolve(data, kernel[None, None, :], mode="same", axes=2),
        dims=data.dims,
        coords=data.coords,
    )

    freqs = data.freqs.data

    n_blocks, n_IMFs, n_freqs = data.shape

    peaks += [freqs[data_sm.argmax("freqs").data]]

    """
    fg.fit(
        freqs=data.freqs.values,
        power_spectra=data.stack(samples=("blocks", "IMFs")).values.T,
        n_jobs=20,
    )

    peaks += [
        np.stack(
            [fg.get_results()[i].peak_params[0, 0] for i in range(n_blocks * n_IMFs)]
        ).reshape(n_blocks, n_IMFs)
    ]
    """


plt.figure(figsize=(20, 20), dpi=600)
for pos in range(15):
    plt.subplot(5, 3, pos + 1)
    for i in range(peaks[pos].shape[1]):
        plt.scatter(peaks[pos][:, i], np.arange(200, dtype=int), s=5)
    plt.ylim(-3, 203)

    plt.xlabel("Frequency [Hz]")
    plt.ylabel("Blocks")
    plt.title(f"Channel {pos + 1}")
plt.tight_layout()
plt.savefig(f"figures/{monkey}/{date}/composites_ps_peaks.png")


plt.figure(figsize=(12, 5))
d = []
for pos in range(40):
    x = peaks[pos].flatten()
    d += [x]
    plt.scatter(x, [pos + 1] * len(x), s=1, c="k")
plt.ylabel("Channel")
plt.xlabel("Frequency [Hz]")


from config import metadata

peaks_sessions = []

for date in tqdm(metadata["monkey"]["FN"]["dates"]):

    ROOT = os.path.expanduser(f"~/funcog/HoffmanData/{monkey}/{date}")
    composite_path = "ps_composite_signals_task_method_eemd_max_imfs_None.nc"

    filepath = os.path.join(ROOT, composite_path)

    composites = xr.load_dataset(filepath)

    channels = list(composites.keys())

    peaks = []

    fg = fooof.FOOOFGroup(max_n_peaks=1, verbose=False)

    for channel in channels:
        data = composites[channel].dropna("IMFs")
        data_sm = xr.DataArray(
            scipy.signal.fftconvolve(data, kernel[None, None, :], mode="same", axes=2),
            dims=data.dims,
            coords=data.coords,
        )

        freqs = data.freqs.data

        n_blocks, n_IMFs, n_freqs = data.shape

        peaks += [freqs[data_sm.argmax("freqs").data]]
    peaks_sessions += [peaks]


d = []
for peaks in peaks_sessions:
    for peak in peaks:
        d += [peak.flatten()]


plt.figure(figsize=(12, 3))
ax = sns.histplot(np.hstack(d), bins=150)
plt.vlines(10, 0, 200000, color="r")
plt.vlines(25, 0, 200000, color="r")
plt.vlines(55, 0, 200000, color="r")


IMFs
