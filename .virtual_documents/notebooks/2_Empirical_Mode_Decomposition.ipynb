


import sys

sys.path.insert(1, "/home/vinicius/storage1/projects/vanderbilt")


import os

import emd
import matplotlib.pyplot as plt
import numpy as np
import PyEMD
import skimage as ski
import umap
import xarray as xr
from frites.utils import parallel_func
from mne.time_frequency import (
    psd_array_multitaper,
    psd_array_welch,
    tfr_array_morlet,
    tfr_array_multitaper,
)
from scipy.optimize import curve_fit
from skimage.segmentation import watershed
from tqdm import tqdm

from VUDA.emd import emd_vec
from VUDA.io.loadbinary import LoadBinary





import logging
from functools import partial

from frites.utils import parallel_func


def _emd(
    data: np.ndarray,
    trials: int,
    max_imf: int = 10,
    block_size=None,
    n_jobs: int = 1,
    verbose: bool = False,
    seed: int = 0,
    use_min_block_size: bool = False,
    method="emd",
):

    assert method in ["emd", "eemd"]

    if method == "emd":
        logging.warning("For method EMD, trials is set to 1.")
        trials = 1

    n_samples = data.shape[0]

    _emd = PyEMD.EEMD(trials=trials)
    _emd.noise_seed(seed)

    if method == "emd":
        f_emd = partial(_emd.emd, T=None, max_imf=max_imf)
    else:
        f_emd = partial(_emd.eemd, progress=False, max_imf=max_imf)

    if isinstance(block_size, int) and (block_size > 1):
        blocks = get_data_blocks(n_samples, block_size, use_min_block_size)
    else:
        blocks = [np.arange(n_samples)]

    nblocks = len(blocks)

    def _for_block(block):

        return f_emd(data[block])

    # define the function to compute in parallel
    parallel, p_fun = parallel_func(
        _for_block, verbose=verbose, n_jobs=n_jobs, total=nblocks
    )

    out = parallel(p_fun(block) for block in blocks)

    return out


def get_composite_signal(
    IMFs: list,
    sfreq: int,
    cutoff: int,
    fmin=0,
    fmax=300,
    bandwidth=4,
    n_jobs=1,
    verbose: bool = False,
):

    n_trials = len(IMFs)

    __iter = range(ntrials)

    filtered = []
    psds = []
    freqs = []

    def _for_trial(i):

        psd, f = psd_array_multitaper(
            IMFs[i],
            fmin=fmin,
            fmax=fmax,
            sfreq=sfreq,
            verbose=False,
            bandwidth=bandwidth,
            n_jobs=n_jobs,
        )

        psd = np.log(psd)

        freqs += [f]
        idx = np.where(f[psd.argmax(axis=1)] > cutoff)[0]
        filtered += [IMFs[i][idx].mean(0)]
        psds += [psd[idx].mean(0)]

    for i in __iter:

        psd, f = psd_array_multitaper(
            IMFs[i],
            fmin=fmin,
            fmax=fmax,
            sfreq=sfreq,
            verbose=False,
            bandwidth=bandwidth,
            n_jobs=n_jobs,
        )

        psd = np.log(psd)

        freqs += [f]
        idx = np.where(f[psd.argmax(axis=1)] > 40)[0]
        filtered += [IMFs[i][idx].mean(0)]
        psds += [psd[idx].mean(0)]


from functools import partial


def get_extreme_size(x, f_max_min=None):
    return f_max_min([x_.shape[0] for x_ in x])


get_min_size = partial(get_extreme_size, f_max_min=np.min)
get_max_size = partial(get_extreme_size, f_max_min=np.max)


def get_data_blocks(n_samples, block_size, use_min_block_size):
    blocks = np.array_split(np.arange(n_samples), block_size)
    if use_min_block_size:
        min_block_size = get_min_size(blocks)
        blocks = np.array([block[:min_block_size] for block in blocks])
    return blocks


def emd_vec(
    x,
    times,
    method="emd",
    max_imfs=None,
    nensembles=None,
    block_size=None,
    verbose=False,
    use_min_block_size=False,
    imf_opts={},
    n_jobs=1,
):

    assert method in ["emd", "eemd"]

    f_emd = dict(emd=emd.sift.sift, eemd=emd.sift.ensemble_sift)
    f_emd_args = dict(max_imfs=max_imfs, imf_opts=imf_opts)

    if method == "emd":
        logging.warning("For method EMD, ensemble size is set to 1.")
        nensembles = 1

    n_samples = x.shape[0]

    if isinstance(block_size, int) and (block_size > 1):
        blocks = get_data_blocks(n_samples, block_size, use_min_block_size)
        block_times = np.vstack(
            [(times[block].min(), times[block].max()) for block in blocks]
        )
    else:
        blocks = [np.arange(n_samples)]

    nblocks = len(blocks)

    # Get start time and end time of each block
    [(times[block].min(), times[block].max()) for block in blocks]

    if method == "emd":
        f_emd = partial(f_emd[method], **f_emd_args)
    else:
        f_emd = partial(f_emd[method], nensembles=nensembles, **f_emd_args)

    def _for_block(block):
        """Perform EMD on a vector."""
        # extract single trial imf
        imf = f_emd(x[block]).T
        imf = xr.DataArray(imf, dims=("IMFs", "times"),)
        return imf

    # define the function to compute in parallel
    parallel, p_fun = parallel_func(
        _for_block, verbose=verbose, n_jobs=n_jobs, total=nblocks
    )

    out = parallel(p_fun(block) for block in blocks)

    out = xr.concat( out, "blocks")

    out.attrs["block_times"] = block_times

    return out





date = "10-20-2022"
monkey = "FN"


filepath = os.path.expanduser(f"~/funcog/HoffmanData/{monkey}/{date}/aHPC_B_cnct.nc")


data = xr.load_dataarray(filepath)


# Get zero timestamp
t_init = data.times.data[0]
# Get final timestamp
t_end = data.times.data[0]
# End of treehouse
t_th_end = float(data.attrs["TH_end"].split(", ")[1])
# Beggining of sleep
t_sleep_init = float(data.attrs["Sleep_start"].split(", ")[1])


data = data.sel(times=slice(t_init, t_th_end))


times = data.times.values





IMFs_single = emd_vec(
    data.sel(channels=33).values,
    times,
    method="eemd",
    max_imfs=9,
    block_size=500,
    nensembles=5,
    use_min_block_size=True,
    n_jobs=20,
    imf_opts={"stop_method": "fixed", "max_iters": 5},
)


IMFs_single = np.stack(IMFs_single)


plt.figure(figsize=(20, 4))
ax = plt.subplot(111)
data.sel(channels=33).plot()
[ax.spines[key].set_visible(False) for key in ["top", "right"]];


imf = IMFs_single[10]

z_imf = (imf - imf.mean(1)[:, None]) / imf.std(1)[:, None]


fig, axd = plt.subplot_mosaic(
    [["A", "A", "C", "C"], ["B", "B", "C", "C"], ["B", "B", "C", "C"]],
    layout="constrained",
    figsize=(10, 5),
    dpi=600,
)

# Plot the signal snippet
plt.sca(axd["A"])
plt.plot(data.sel(channels=33).data)
plt.xlim(0, 5000)
plt.xlabel("Time [a.u]")
plt.ylabel(r"LFP [$\mu$V]")
[axd["A"].spines[key].set_visible(False) for key in ["top", "right"]]

# Plot the signal IMFs
plt.sca(axd["B"])
colors = []
for i in range(imf.shape[0]):
    p_ = plt.plot(z_imf[i] + 14.5 * (z_imf.shape[0] - i))
    colors += [p_[0].get_color()]
    plt.text(
        -490,
        14.5 * (z_imf.shape[0] - i),
        f"IMF {i + 1}",
        color=colors[-1],
        rotation=20,
    )
    plt.axis("off")


psd, f = psd_array_multitaper(
    imf,
    fmin=0,
    fmax=300,
    sfreq=1000,
    bandwidth=4,
    verbose=False,
    n_jobs=10,
)

plt.sca(axd["C"])
for i in range(len(psd)):
    plt.semilogx(f, psd[i] / psd[i].sum())
plt.xlabel("frequency [Hz]")
plt.ylabel("Norm. PSD")
[axd["C"].spines[key].set_visible(False) for key in ["top", "right"]];





import xarray as xr
import pandas as pd
import numpy as np

# Create some sample data
data1 = np.random.rand(3, 4)
data2 = np.random.rand(3, 5)
data3 = np.random.rand(3, 6)

# Create DataArrays with different sizes
array1 = xr.DataArray(data1, dims=('dim1', 'dim2'), coords={'dim1': range(3), 'dim2': range(4)})
array2 = xr.DataArray(data2, dims=('dim1', 'dim2'), coords={'dim1': range(3), 'dim2': range(5)})
array3 = xr.DataArray(data3, dims=('dim1', 'dim2'), coords={'dim1': range(3), 'dim2': range(6)})

# Create a Dataset and add DataArrays to it
dataset = xr.Dataset({
    'data_array1': array1,
    'data_array2': array2,
    'data_array3': array3,
})

# Display the resulting Dataset
print(dataset)



import jax
import jax.numpy as jnp


def jax_euclidean(matrix):
    def _for_each(carry, row):
        return carry, jnp.sqrt(jnp.sum((row - matrix) ** 2, axis=1))

    _, result = jax.lax.scan(_for_each, None, matrix)
    return result


# Example usage:
data_matrix = jnp.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

result = jax_euclidean(data_matrix)
print("Pairwise Euclidean Distances:")
print(result)


SXX, f = psd_array_multitaper(
    IMFs_single[:, 1:, :],
    fmin=0,
    fmax=300,
    sfreq=1000,
    verbose=True,
    bandwidth=4,
    n_jobs=20,
)


SXX = SXX.reshape(8 * 500, -1)


SXX_norm = SXX / SXX.sum(1)[:, None]


reducer = umap.UMAP(n_jobs=20, min_dist=.5, n_neighbors=5)
embedding = reducer.fit_transform(SXX_norm)


from sklearn.cluster import DBSCAN, KMeans

clustering = DBSCAN(
    eps=.04, min_samples=1, n_jobs=20
).fit(SXX_norm)


plt.scatter(embedding[:, 0], embedding[:, 1], s=.5, c=clustering.labels_, cmap="tab10")


unique_labels = np.unique(clustering.labels_)


n_cluster = len(unique_labels)


def average_for_cluster(data=None, cluster_labels=None, label=None):
    return data[cluster_labels == label].mean(0)

partial_average_for_cluster = partial(
    average_for_cluster, data=SXX_norm, cluster_labels=clustering.labels_
)

out = np.stack([partial_average_for_cluster(label=label) for label in unique_labels])

order = np.argsort(out.argmax(axis=1))


unique_labels[order]


plt.figure(figsize=(8, 5))
for pos, i in enumerate(unique_labels[order]):
    plt.subplot(2, n_cluster // 2, pos + 1)
    plt.semilogx(f, SXX_norm[clustering.labels_ == i].T, lw=.1, color="b")
    plt.semilogx(f, SXX_norm[clustering.labels_ == i].mean(0), lw=2, color="k")
    plt.xlabel("Frequency [Hz]")
    plt.ylabel("Norm. POWER")
    #plt.title(titles[pos])
plt.tight_layout()


IMFs_single














def mt_decomposition(
    data, sfreq, fmin=0, fmax=np.inf, bandwidth=4, verbose=False, n_jobs=1
):
    """
    Perform multitaper decomposition on input data.

    Parameters:
    - data (numpy.ndarray): Input data with shape (n_channels, n_blocks, n_imfs, n_times).
    - sfreq (float): Sampling frequency of the data.
    - fmin (float, optional): Minimum frequency for PSD computation (default: 0).
    - fmax (float, optional): Maximum frequency for PSD computation (default: np.inf).
    - bandwidth (int, optional): Bandwidth parameter for multitaper decomposition (default: 4).
    - verbose (bool, optional): Flag to enable/disable verbose output (default: False).
    - n_jobs (int, optional): Number of parallel jobs to run (default: 1).

    Returns:
    - xr.DataArray: Multitaper PSD results with dimensions ('channels', 'blocks', 'IMF', 'freqs').
                   The 'freqs' coordinate represents the mean frequency vector.

    Note:
    This function uses parallel processing to compute the power spectral density (PSD)
    for each sample in the input data. The results are stacked to form a DataArray.

    Example:
    ```python
    data = ...  # provide input data
    sfreq = 1000  # set sampling frequency
    result = mt_decomposition(data, sfreq)
    print(result)
    ```
    
    """

    n_blocks, n_imfs, n_times = data.shape

    def _for_block(i):
        return psd_array_multitaper(
            data[i],
            fmin=fmin,
            fmax=fmax,
            sfreq=sfreq,
            verbose=False,
            bandwidth=bandwidth,
            n_jobs=1,
        )

    # define the function to compute in parallel
    parallel, p_fun = parallel_func(
        _for_sample, verbose=verbose, n_jobs=n_jobs, total=n_samples
    )

    out = parallel(p_fun(i) for i in range(n_samples))

    fvec = np.stack([out[i][1] for i in range(n_samples)]).mean(0)
    
    SXX = np.stack([out[i][0] for i in range(n_samples)]).reshape(
        n_channels, n_blocks, n_imfs, len(fvec)
    )

    SXX = xr.DataArray(
        SXX, dims=("channels", "blocks", "IMF", "freqs"), coords={"freqs": fvec}
    )
    
    return SXX


out = mt_decomposition(IMFs_single, 1000, fmin=0, fmax=300, bandwidth=2, n_jobs=40)


idx = np.random.choice(out.channels.values, 10, replace=False)


SXX = out.sel(channels=idx).stack(IMFs=("channels", "blocks", "IMF")).T.values


SXX_norm = SXX / SXX.sum(1)[:, None]


reducer = umap.UMAP(n_jobs=20, min_dist=.5, n_neighbors=5)
embedding = reducer.fit_transform(SXX_norm)





from fooof import FOOOFGroup

fg = FOOOFGroup(max_n_peaks=1)
fg.fit(f, SXX_norm, [f.min(), f.max()])


colors = []
for i in range(len(fg.get_results())):
    try:
        colors += [fg.get_results()[i].peak_params.squeeze()[0]]
    except:
        colors += [0]


import numpy as np
from sklearn.cluster import KMeans

clustering = KMeans(n_clusters=4, n_init='auto')
cluster_labels = clustering.fit_predict(SXX_norm)


plt.scatter( embedding[:, 0], embedding[:, 1], s=.1, c=cluster_labels, cmap="tab10" )


from sklearn.cluster import DBSCAN

clustering = DBSCAN(eps=0.03, min_samples=20, n_jobs=20)

cluster_labels = clustering.fit_predict(SXX_norm)


plt.scatter( embedding[:, 0], embedding[:, 1], s=.1, c=cluster_labels, cmap="tab10" )


from sklearn.cluster import AffinityPropagation
import numpy as np
X = np.array([[1, 2], [1, 4], [1, 0],
              [4, 2], [4, 4], [4, 0]])
clustering = AffinityPropagation(damping=.5)
cluster_labels = clustering.fit_predict(SXX_norm)


plt.scatter( embedding[:, 0], embedding[:, 1], s=.1, c=cluster_labels, cmap="tab10" )


cluster_labels += 1


unique_labels = np.unique(cluster_labels)
n_clusters = len(unique_labels)
print(n_clusters, unique_labels)














order


titles = ["SLOW 1", "SLOW 2", "SLOW 3", "MEDIUM 1", "MEDIUM 2", "FAST"]


print(unique_labels[order])


plt.figure(figsize=(12, 5))
for pos, i in enumerate(unique_labels[order]):
    plt.subplot(3, 3, pos + 1)
    #  plt.semilogx(f, SXX_norm[cluster_labels == i].T, lw=0.1, color="b")
    plt.semilogx(f, SXX_norm[cluster_labels == i].mean(0), lw=2, color="k")
    plt.xlabel("Frequency [Hz]")
    plt.ylabel("Norm. POWER")
    #plt.title(titles[pos])
plt.tight_layout()








from functools import partial

import jax
import jax.numpy as jnp


def get_bc(cycles, freq, k_sd=5):
    return cycles / (k_sd * freq)


def cxmorelet(freq, cycles, sampling_freq):
    t = jnp.linspace(-1, 1, sampling_freq * 2)

    bc = get_bc(cycles, freq)
    norm = 1 / (bc * jnp.sqrt(2 * jnp.pi))
    gauss = jnp.exp(-(t**2) / (2 * bc**2))
    sine = jnp.exp(1j * 2 * jnp.pi * freq * t)

    wavelet = norm * gauss * sine
    return wavelet / jnp.sum(jnp.abs(wavelet))


@partial(jax.jit, static_argnums=3)
@partial(jax.vmap, in_axes=(None, 0, None, None))
def wavelet_transform(signal, freq, cycles, sampling_freq):
    wavelet = cxmorelet(freq, cycles, sampling_freq)
    return jax.scipy.signal.convolve(signal, wavelet, mode="same")


@partial(jax.jit, static_argnums=3)
@partial(jax.vmap, in_axes=(None, None, 0, None))
def superlet_transform_helper(signal, freqs, order, sampling_freq):
    return wavelet_transform(signal, freqs, order, sampling_freq) * jnp.sqrt(2)


def order_to_cycles(base_cycle, max_order, mode):
    if mode == "add":
        return jnp.arange(0, max_order) + base_cycle
    elif mode == "mul":
        return jnp.arange(1, max_order + 1) * base_cycle
    else:
        raise ValueError('mode should be one of "mul" or "add"')


def get_order(f, f_min: int, f_max: int, o_min: int, o_max: int):
    return o_min + jnp.round((o_max - o_min) * (f - f_min) / (f_max - f_min))


@partial(jax.vmap, in_axes=(0, None))
def get_mask(order, max_order):
    return jnp.arange(1, max_order + 1) > order


@jax.jit
def norm_geomean(X, root_pows, eps):
    X = jnp.log(X + eps).sum(axis=0)

    return jnp.exp(X / jnp.array(root_pows).reshape(-1, 1))


# @jax.jit
def adaptive_superlet_transform(
    signal,
    freqs,
    sampling_freq: int,
    base_cycle: int,
    min_order: int,
    max_order: int,
    eps=1e-12,
    mode="mul",
):
    """Computes the adaptive superlet transform of the provided signal
    Args:
        signal (jnp.ndarray): 1D array containing the signal data
        freqs (jnp.ndarray): 1D sorted array containing the frequencies to compute the wavelets at
        sampling_freq (int): Sampling frequency of the signal
        base_cycle (int): The number of cycles corresponding to order=1
        min_order (int): The minimum upper limit of orders to be used for a frequency in the adaptive superlet.
        max_order (int): The maximum upper limit of orders to be used for a frequency in the adaptive superlet.

        eps (float, optional): Epsilon value to be used for numerical stability in the geometric mean. Defaults to 1e-12.
        mode (str, optional): "add" or "mul", corresponding to the use of additive or multiplicative adaptive superlets. Defaults to "mul".
    Returns:
        jnp.ndarray: 2D array (Frequency x Time) representing the computed scalogram
    """
    cycles = order_to_cycles(base_cycle, max_order, mode)
    orders = get_order(freqs, min(freqs), max(freqs), min_order, max_order)
    mask = get_mask(orders, max_order)

    out = superlet_transform_helper(signal, freqs, cycles, sampling_freq)
    out = out.at[mask.T].set(1)

    return norm_geomean(out, orders, eps)


def superlets(
    data,
    freqs,
    sampling_freq,
    base_cycle=3,
    min_order=1,
    max_order=30,
    eps=1e-12,
    mode="mul",
):
    if not isinstance(data, jax.numpy.ndarray):
        data = jnp.array(data)
    if not isinstance(freqs, jax.numpy.ndarray):
        freqs = jnp.array(freqs)

    _fcn = partial(
        adaptive_superlet_transform,
        freqs=freqs,
        sampling_freq=sampling_freq,
        base_cycle=base_cycle,
        min_order=min_order,
        max_order=max_order,
        eps=eps,
        mode=mode,
    )
    tf = jax.vmap(jax.vmap(_fcn, in_axes=(0,)), in_axes=(0,))
    return tf(data)


freqs = np.arange(1, 20, 1)

slow1 = IMFs_single[0][cluster_labels[:9] == 5].mean(0)
slow2 = IMFs_single[0][cluster_labels[:9] == 4].mean(0)

slow1 = (slow1 - slow1.mean()) / slow1.std()
slow2 = (slow2 - slow2.mean()) / slow2.std()

out1 = adaptive_superlet_transform(
    slow1,
    freqs,
    sampling_freq=1000,
    base_cycle=3,
    min_order=1,
    max_order=20,
    eps=1e-12,
    mode="mul",
)

out2 = adaptive_superlet_transform(
    slow2 + slow1,
    freqs,
    sampling_freq=1000,
    base_cycle=3,
    min_order=1,
    max_order=30,
    eps=1e-12,
    mode="mul",
)


plt.imshow(
    (out1 * np.conj(out1)).real,
    aspect="auto",
    cmap="turbo",
    origin="lower",
    vmin=0,
    vmax=0.6,
)

plt.plot(slow1 + 7, color="w", lw=1)
plt.yticks(np.arange(len(freqs)), np.round(freqs, 2))
plt.ylabel("Frequency [Hz]")
plt.xlabel("Time")
plt.colorbar()
plt.title("SLOW 1")


plt.imshow(
    (out2 * np.conj(out2)).real,
    aspect="auto",
    cmap="turbo",
    origin="lower",
    vmin=0,
    vmax=1,
)

plt.plot(slow2 + 7, color="w")
plt.yticks(range(len(freqs)), np.round(freqs, 2))
plt.ylabel("Frequency [Hz]")
plt.xlabel("Time")
plt.colorbar()
plt.title("SLOW1 + SLOW 2")


out3 = adaptive_superlet_transform(
    data.sel(channels=33)[:9774].data,
    freqs,
    sampling_freq=1000,
    base_cycle=3,
    min_order=1,
    max_order=30,
    eps=1e-12,
    mode="mul",
)


plt.imshow(
    (out3 * np.conj(out3)).real,
    aspect="auto",
    cmap="turbo",
    origin="lower",
    vmin=0,
    vmax=3000,
)
plt.colorbar()
plt.yticks(range(len(freqs)), np.round(freqs, 2))
plt.ylabel("Frequency [Hz]")
plt.xlabel("Time")


mt1, fvec1 = psd_array_multitaper(
    slow1, 1000, fmin=0, bandwidth=4, fmax=300, output="power"
)
mt2, fvec2 = psd_array_multitaper(
    slow2, 1000, fmin=0, bandwidth=4, fmax=300, output="power"
)
plt.semilogx(fvec1, mt1)
plt.semilogx(fvec2, mt2)


plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.scatter(x=embedding[:, 0], y=embedding[:, 1], c=colors, cmap="jet", s=3)
cbar = plt.colorbar()
cbar.ax.set_ylabel("Frequency [Hz]", rotation=270, labelpad=15)

plt.subplot(1, 2, 2)
plt.scatter(x=embedding[:, 0], y=embedding[:, 1], c=cluster_labels, cmap="jet", s=3)
cbar = plt.colorbar()
cbar.ax.set_ylabel("KMEANS labels", rotation=270, labelpad=15)





from scipy.signal import find_peaks


find_peaks(SXX[0], prominence=1)





plt.plot(SXX.freqs, SXX[1])


A = np.corrcoef(IMFs_single[0])
np.fill_diagonal(A, 0)
plt.imshow(A, vmin=-0.1, vmax=0.1, cmap="RdBu_r")
plt.colorbar()








x = data.sel(channels=33).squeeze().data[:14160]

_emd = PyEMD.EEMD(trials=50)
_emd.noise_seed(1234546)

imf = _emd.eemd(x, progress=False, max_imf=None)

z_imf = (imf - imf.mean(1)[:, None]) / imf.std(1)[:, None]


fig, axd = plt.subplot_mosaic(
    [["A", "A", "C", "C"], ["B", "B", "C", "C"], ["B", "B", "C", "C"]],
    layout="constrained",
    figsize=(10, 5),
    dpi=600,
)

# Plot the signal snippet
plt.sca(axd["A"])
plt.plot(x)
plt.xlim(0, 5000)
plt.xlabel("Time [a.u]")
plt.ylabel(r"LFP [$\mu$V]")
[axd["A"].spines[key].set_visible(False) for key in ["top", "right"]]

# Plot the signal IMFs
plt.sca(axd["B"])
colors = []
for i in range(imf.shape[0]):
    p_ = plt.plot(z_imf[i] + 14.5 * (z_imf.shape[0] - i))
    colors += [p_[0].get_color()]
    plt.text(
        -490,
        14.5 * (z_imf.shape[0] - i),
        f"IMF {i + 1}",
        color=colors[-1],
        rotation=20,
    )
    plt.axis("off")


psd, f = psd_array_multitaper(
    imf,
    fmin=0,
    fmax=300,
    sfreq=1000,
    verbose=False,
    bandwidth=12,
    n_jobs=10,
)

plt.sca(axd["C"])
for i in range(len(psd)):
    plt.semilogx(f, psd[i] / psd[i].sum())
plt.xlabel("frequency [Hz]")
plt.ylabel("Norm. PSD")
[axd["C"].spines[key].set_visible(False) for key in ["top", "right"]]

plt.savefig("figures/emf_snippet_example.png")





def gauss(x, a, x0, sigma):
    return a * np.exp(-((x - x0) ** 2) / (2 * sigma**2))


def gaussian_fit(y, x):
    try:
        popt, pcov = curve_fit(gauss, x, y, p0=[1, x[y.argmax()], 1])
    except:
        popt = np.nan
    return popt


"""
theta_composite = []
gamma_composite = []

for i in tqdm(range(1)):

    psd, f = psd_array_multitaper(
        IMFs[i], fmin=0, fmax=300, sfreq=1000, verbose=False, bandwidth=4, n_jobs=30
    )

    # Fit a gaussian to each spectrum and get their first momentum
    popt = np.apply_along_axis(gaussian_fit, 1, psd, f)
    mu = popt[:, 1]

    # theta composite
    theta_peaks = np.logical_and(mu >= 3, mu < 8)
    idx = np.where(theta_peaks)[0]
    theta_composite.append(IMFs[i][idx].sum(0))
    # gamma composite
    gamma_peaks = np.logical_and(mu >= 40, mu < 150)
    idx = np.where(gamma_peaks)[0]
    gamma_composite.append(IMFs[i][idx].sum(0))
""";


def _for_batch(IMF: list, bands: list, band_names: list = None):

    n_imfs, n_times = IMF.shape
    n_bands = len(bands)

    if not isinstance(band_names, (list, np.ndarray)):
        band_names = np.median(bands, axis=1).astype(int)

    psd, f = psd_array_multitaper(
        IMF, fmin=0, fmax=300, sfreq=1000, verbose=False, bandwidth=4, n_jobs=1
    )

    # Fit a gaussian to each spectrum and get their first momentum
    popt = np.apply_along_axis(gaussian_fit, 1, psd, f)
    mu = popt[:, 1]

    # Composite signals
    composite = np.empty((n_bands, n_times))

    for pos, (f_l, f_h) in enumerate(bands):
        peaks = np.logical_and(mu >= f_l, mu < f_h)
        idx = np.where(peaks)[0]
        composite[pos] = IMF[idx].sum(0)

    dims = ("bands", "times")
    coords = {"bands": band_names}

    composite = xr.DataArray(composite, dims=dims, coords=coords)

    return composite


# define the function to compute in parallel
parallel, p_fun = parallel_func(_for_batch, n_jobs=30, verbose=False, total=len(IMFs))
# Compute the single trial coherence
composite = parallel(p_fun(IMF, [[3, 8], [40, 150]]) for IMF in IMFs)


lengths = np.asarray([signal.shape[1] for signal in composite])


composite = xr.concat([signal[:, 0 : lengths.min()] for signal in composite], "batches")


plt.figure(figsize=(8, 4))
ax = plt.subplot(111)
plt.plot(np.squeeze(composite[100][1]), "k", label=r"$\gamma$")
plt.plot(np.squeeze(composite[100][0]), "r", lw=3, label=r"$\theta$")
plt.legend()
[ax.spines[key].set_visible(False) for key in ["top", "right"]]
plt.xlim(2500, 4500)
plt.xlabel("Time")





freqs = np.linspace(3, 10, 50)

W_theta = tfr_array_morlet(
    composite.isel(bands=[0]),
    1000,
    freqs,
    n_cycles=12,
    n_jobs=30,
).squeeze()

dims = ("batches", "freqs", "times")
coords = dict(freqs=freqs)
W_theta = xr.DataArray((W_theta * W_theta.conj()).real, dims=dims, coords=coords)


freqs = np.linspace(40, 150, 50)

W_gamma = tfr_array_morlet(
    composite.isel(bands=[1]),
    1000,
    freqs,
    n_cycles=freqs / 2,
    n_jobs=30,
).squeeze()

dims = ("batches", "freqs", "times")
coords = dict(freqs=freqs)
W_gamma = xr.DataArray((W_gamma * W_gamma.conj()).real, dims=dims, coords=coords)


x = composite.isel(bands=0, batches=0)
y = composite.isel(bands=1, batches=0)

x = (x - x.mean("times")) / (7 * x.std("times"))
y = (y - y.mean("times")) / (y.std("times"))


plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
W_theta[0].plot(cmap="turbo", vmax=1e6)
plt.plot(x + 9, "w")
plt.title(r"$\theta$-composite")
plt.xlim(5000, 8200)

plt.subplot(1, 2, 2)
W_gamma[0].plot(cmap="turbo", vmax=120000)
plt.plot(y + 133, "w")
plt.title(r"$\gamma$-composite")
plt.xlim(2500, 5000)





def return_labeled_image(img: list, threshold: float):
    """
    Label regions in a binary image using a given threshold.

    Parameters:
    - img (numpy.ndarray): The binary image to label.
    - threshold (float): The threshold for labeling.

    Returns:
    - numpy.ndarray: A labeled image with connected regions.
    - numpy.ndarray: Array of unique labels.
    - int: The number of unique labels.

    This function labels connected regions in a binary image based on a given threshold.
    It uses the `ski.measure.label` function from the scikit-image library to perform
    the labeling. The resulting labeled image contains connected regions with unique
    labels, and the number of labels is also returned.

    Example:
    labeld_image, labels, nlabels = return_labeled_image(binary_img, 0.5)
    """
    # Labeled image
    labeld_image = ski.measure.label(img > threshold, background=0)
    # Get unique labels
    labels = np.unique(labeld_image)[1:]
    # Number of labels
    nlabels = len(labels)

    return labeld_image, labels, nlabels


def detect_burts(
    spectra: xr.DataArray,
    init_threshold: float,
    min_threshold: float,
    gamma: float,
    relative: bool = True,
):
    """
    Detect bursts in spectra using a dynamic thresholding approach.

    Parameters:
    - spectra (numpy.ndarray): The input spectra to analyze.
    - init_threshold (float): The initial threshold for labeling.
    - min_threshold (float): The minimum threshold to stop the labeling process.

    Returns:
    - numpy.ndarray: An image with labeled bursts.

    This function detects bursts in a given spectra by iteratively updating a labeling
    based on threshold values. It starts with an initial threshold, and in each iteration,
    it updates the labeling using a lower threshold. The process continues until the
    threshold reaches the minimum threshold value. The resulting labeled image contains
    burst regions.

    Note: The input spectra are first z-scored before applying labeling.

    Example:
    labeled_bursts = detect_burts(spectra_data, 2.0, 1.0)
    """
    # Dimensions of the spectra
    size = spectra.shape
    # z-score spectra
    if relative:
        z = (spectra - spectra.mean("times")) / spectra.std("times")
    else:
        z = (spectra - spectra.mean()) / spectra.std()
    # label image using initial threshold
    labeled_image, labels, nlabels = return_labeled_image(z, init_threshold)

    # Update threshold
    thr = init_threshold - gamma

    while thr >= min_threshold:

        # Label image for new threshold
        new_labeled_image, new_labels, new_nlabels = return_labeled_image(z, thr)
        # Work with the flattened matrix
        new_labeled_image = new_labeled_image.reshape(-1)

        # Copy original image
        temp = labeled_image.copy().reshape(-1)
        # Get biggest label
        max_label = labels.max()

        # Check mergings of burts
        for nl in new_labels:
            # For a given label in the new labeled image
            index_nl = np.where(new_labeled_image == nl)[0]
            # Check if in the previous one it corresponded to two or more burts
            if len(np.unique(temp[index_nl])) > 2:
                # If yes, keep old labeling
                new_labeled_image[index_nl] = temp[index_nl]

        # Check if new labels contain old ones
        for nl in new_labels:
            # For a given label in the new labeled image
            intersection = []
            index_nl = np.where(new_labeled_image == nl)[0]
            # For a given label in the old labeled image
            for l in labels:
                # Keep the indexes where new-contains-old labels
                index_l = np.where(labeled_image == l)[0]
                if len(np.isin(index_l, index_nl)):
                    intersection.append(l)

            # Update region of intersect in case any was found
            if len(intersection) > 0:
                intersection = np.hstack(intersection)
                indexes = np.where(np.intersect1d(temp, intersection))
                temp[index_nl] = intersection.max()
            else:
                max_label = max_label + 1
                temp[index_nl] = max_label

        # Update labeled image
        labeled_image, labels, nlabels = new_labeled_image, new_labels, new_nlabels
        # Update threshold
        thr = thr - gamma

    # Reset labels in the final image
    labeled_image, labels, nlabels = return_labeled_image(
        labeled_image.reshape(size), 0
    )

    return labeled_image


z = W_theta[0]
z = (z - z.mean()) / z.std()
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.imshow(z, aspect="auto", origin="lower", cmap="turbo", vmax=5)
plt.colorbar()
plt.subplot(1, 2, 2)
plt.imshow(
    detect_burts(W_theta[0], 5, 1, 0.1) > 0,
    aspect="auto",
    origin="lower",
    cmap="binary_r",
)


z = W_gamma[0]
z = (z - z.mean()) / z.std()
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.imshow(z, aspect="auto", origin="lower", cmap="turbo", vmax=5)
plt.subplot(1, 2, 2)
plt.imshow(
    detect_burts(W_gamma[0], 6, 2, 0.1) > 0,
    aspect="auto",
    origin="lower",
    cmap="binary_r",
)


pos = 0
z = (W_gamma[pos] - W_gamma[pos].std()) / W_gamma[pos].std()


plt.imshow(W_gamma[pos], origin="lower", aspect="auto", cmap="turbo")
plt.title("spectogram")
plt.savefig("figures/frame1.png", dpi=300)


plt.imshow(
    z,
    origin="lower",
    aspect="auto",
    cmap="turbo",
)
plt.title("z-scored spectogram")
plt.savefig("figures/frame2.png", dpi=300)


frame = 3
for thr in [4, 3, 2, 1]:
    mask = detect_burts(W_gamma[pos], 5, thr, 0.1) > 0
    x = np.nan * (z * mask) + z
    plt.imshow(
        z * (np.where(mask, np.nan, mask) + 1),
        origin="lower",
        aspect="auto",
        cmap="turbo",
    )
    plt.title(f"threshold = {thr} STDs")
    plt.savefig(f"figures/frame{frame}.png", dpi=300)
    plt.close()
    frame = frame + 1
plt.tight_layout()





def extract_features(W, labeled_bursts):

    unique_labels = np.unique(labeled_bursts)[1:]
    nlabels = len(unique_labels)

    # Avg. and STD of each burst
    mean_amplitude = np.empty(nlabels)
    std_amplitude = np.empty(nlabels)

    # Avg., STD of the frequency of each burst and their weighted version
    mean_freq = np.empty(nlabels)
    std_freq = np.empty(nlabels)
    w_mean_freq = np.empty(nlabels)
    w_std_freq = np.empty(nlabels)
    peak_freq = np.empty(nlabels)

    # Avg., STD of the time of each burst and their weighted version
    mean_time = np.empty(nlabels)
    std_time = np.empty(nlabels)
    w_mean_time = np.empty(nlabels)
    w_std_time = np.empty(nlabels)
    t_start = np.empty(nlabels)
    t_stop = np.empty(nlabels)
    duration = np.empty(nlabels)

    labels_flattened = labeled_bursts.reshape(-1)
    W_stacked = W.stack(flat=("freqs", "times"))

    times = W_stacked.times.data
    freqs = W_stacked.freqs.data

    for pos, label in enumerate(unique_labels):
        mean_amplitude[pos] = W_stacked[labels_flattened == label].mean()
        std_amplitude[pos] = W_stacked[labels_flattened == label].std()

        mean_freq[pos] = freqs[labels_flattened == label].mean()
        std_freq[pos] = freqs[labels_flattened == label].std()
        w_mean_freq[pos], w_std_freq[pos] = weighted_avg_and_std(
            freqs[labels_flattened == label], W_stacked[labels_flattened == label]
        )
        peak_freq[pos] = freqs[labels_flattened == label].max()

        mean_time[pos] = times[labels_flattened == label].mean()
        std_time[pos] = times[labels_flattened == label].std()
        w_mean_time[pos], w_std_time[pos] = weighted_avg_and_std(
            times[labels_flattened == label], W_stacked[labels_flattened == label]
        )

        t_start[pos] = times[labels_flattened == label].min()
        t_stop[pos] = times[labels_flattened == label].max()

        duration[pos] = t_stop[pos] - t_start[pos]

    return (
        mean_amplitude,
        std_amplitude,
        mean_freq,
        std_freq,
        w_mean_freq,
        w_std_freq,
        peak_freq,
        mean_time,
        std_time,
        w_mean_time,
        w_std_time,
        t_start,
        t_stop,
        duration,
    )


def _for_batch(W):
    init = int(((W.max("times") - W.mean("times")) / W.std("times")).max().data.item())
    return detect_burts(W, init, 1, 0.1)


# define the function to compute in parallel
parallel, p_fun = parallel_func(_for_batch, verbose=False, n_jobs=20, total=100)

labeled_theta_bursts = parallel(p_fun(W) for W in W_theta[:100])


labeled_theta_bursts = np.stack(labeled_theta_bursts)


def weighted_avg_and_std(values, weights):
    """
    Return the weighted average and standard deviation.

    values, weights -- NumPy ndarrays with the same shape.
    """
    average = np.average(values, weights=weights)
    # Fast and numerically precise:
    variance = np.average((values - average) ** 2, weights=weights)
    return (average, np.sqrt(variance))


feature_names = [
    "mean_amplitude",
    "std_amplitude",
    "mean_freq",
    "std_freq",
    "w_mean_freq",
    "w_std_freq",
    "peak_freq",
    "mean_time",
    "std_time",
    "w_mean_time",
    "w_std_time",
    "t_start",
    "t_stop",
    "duration",
]

features = {}

for name in feature_names:

    features[name] = []

for i in tqdm(range(100)):
    out = extract_features(W_theta[i], labeled_theta_bursts[i])
    for pos, name in enumerate(feature_names):
        features[name] += [out[pos]]


plt.figure(figsize=(12, 8))
pos = 1
for name in feature_names:
    if name not in ["t_start", "t_stop"]:
        plt.subplot(3, 4, pos)
        plt.hist(np.hstack(features[name]), 50)
        plt.title(name)
        pos = pos + 1
plt.tight_layout()


def _for_batch(W):
    init = int(((W.max("times") - W.mean("times")) / W.std("times")).max().data.item())
    return detect_burts(W, init, 1, 0.1)


# define the function to compute in parallel
parallel, p_fun = parallel_func(_for_batch, verbose=False, n_jobs=20, total=100)

labeled_gamma_bursts = parallel(p_fun(W) for W in W_gamma[:100])


labeled_gamma_bursts = np.stack(labeled_gamma_bursts)


features_gamma = {}

for name in feature_names:

    features_gamma[name] = []

for i in tqdm(range(100)):
    out = extract_features(W_gamma[i], labeled_gamma_bursts[i])
    for pos, name in enumerate(feature_names):
        features_gamma[name] += [out[pos]]


plt.figure(figsize=(12, 8))
pos = 1
for name in feature_names:
    if name not in ["t_start", "t_stop"]:
        plt.subplot(3, 4, pos)
        plt.hist(np.hstack(features_gamma[name]), 50)
        plt.title(name)
        pos = pos + 1
plt.tight_layout()


import matplotlib as mpl

plt.figure(figsize=(9, 3.5))

plt.subplot(121)

plt.hist2d(
    np.hstack(features["mean_freq"]),
    np.log(np.hstack(features["mean_amplitude"])),
    bins=(20, 20),
    norm=mpl.colors.LogNorm(),
    cmap="turbo",
)
plt.ylabel("log(Amplitude)")
plt.xlabel("frequency [Hz]")
plt.title(r"$\theta$-bursts")
plt.colorbar()

plt.subplot(122)

plt.hist2d(
    np.hstack(features_gamma["mean_freq"]),
    np.log(np.hstack(features_gamma["mean_amplitude"])),
    bins=(20, 20),
    norm=mpl.colors.LogNorm(),
    cmap="turbo",
)
plt.ylabel("log(Amplitude)")
plt.xlabel("frequency [Hz]")
plt.title(r"$\gamma$-bursts")
plt.colorbar()

plt.tight_layout()


import numba as nb


@nb.njit
def overlaps(theta_timings: list, gamma_timings: list):
    n_theta = len(theta_timings)
    n_gamma = len(gamma_timings)

    n_overlaps = np.empty(n_gamma, dtype=np.int8)

    for i in range(n_gamma):
        temp = np.logical_and(
            theta_timings[:, 0] - gamma_timings[i, 0] < 0,
            theta_timings[:, 1] - gamma_timings[i, 0] > 0,
        )

        temp = np.logical_and(
            temp,
            np.logical_and(
                theta_timings[:, 0] - gamma_timings[i, 1] < 0,
                theta_timings[:, 1] - gamma_timings[i, 1] > 0,
            ),
        )

        n_overlaps[i] = temp.sum()

    return n_overlaps


n_overlaps = []

for i in range(100):

    T_theta = np.stack((features["t_start"][i], features["t_stop"][i]), axis=1)
    T_gamma = np.stack(
        (features_gamma["t_start"][i], features_gamma["t_stop"][i]), axis=1
    )

    n_overlaps += [overlaps(T_theta, T_gamma)]


n_overlaps = np.hstack(n_overlaps)
amplitudes = np.hstack(features_gamma["mean_amplitude"])


plt.figure(figsize=(9, 3.5))

ax = plt.subplot(121)
n, x = np.histogram(n_overlaps, bins=[0, 1, 2, 3, 4])
plt.bar(x[:-1], n / n.sum())
plt.xlabel("#theta bursts per gamma burst")
[ax.spines[key].set_visible(False) for key in ["top", "right"]]
ax = plt.subplot(122)
import pandas as pd
import seaborn as sns

df = pd.DataFrame(
    np.stack((n_overlaps, amplitudes), axis=1), columns=["overlap", "amplitude"]
)
sns.boxplot(data=df, x="overlap", y="amplitude", showfliers=False, color="lightblue")
[ax.spines[key].set_visible(False) for key in ["top", "right"]]

plt.xticks(rotation=90)

plt.tight_layout()


SIZES = []
FREQS = []

for pos in tqdm(range(100)):

    times = W_gamma[pos].stack(flat=("freqs", "times")).times.data
    freqs = W_gamma[pos].stack(flat=("freqs", "times")).freqs.data

    labeled_bursts = labeled_gamma_bursts[pos].reshape(-1)

    unique_labels = np.unique(labeled_bursts)[1:]
    n_labels = len(unique_labels)

    S = np.zeros((n_labels, n_labels))
    F_min = np.zeros(n_labels)
    F_max = np.zeros(n_labels)
    F_mean = np.zeros(n_labels)

    for l_i in unique_labels - 1:
        F_min[l_i] = freqs[labeled_bursts == l_i + 1].min()
        F_max[l_i] = freqs[labeled_bursts == l_i + 1].max()
        F_mean[l_i] = freqs[labeled_bursts == l_i + 1].mean()

    for l_i in range(n_labels - 1):
        times_i = times[labeled_bursts == l_i + 1]
        s_i = len(times_i)
        for l_j in range(l_i, n_labels):
            if F_max[l_i] < F_min[l_j]:
                times_j = times[labeled_bursts == l_j + 1]
                s_j = len(times_j)
                S[l_i, l_j] = len(np.intersect1d(times_i, times_j)) / np.min((s_i, s_j))
    np.fill_diagonal(S, 0)
    SIZES += [S]
    FREQS += [F_mean]


x, y = [], []
for i in range(len(SIZES)):
    pi, pj = np.where(SIZES[i] > 0.95)
    x += [FREQS[i][pi]]
    y += [FREQS[i][pj]]

x = np.hstack(x)
y = np.hstack(y)
plt.hist2d(x, y, cmap="hot_r")
plt.xlabel("Frequency [Hz]")
plt.ylabel("Frequency [Hz]")
plt.title(r"Density of harmonic $\gamma$ bursts")
plt.colorbar()


SIZES = []
FREQS = []

for pos in tqdm(range(100)):

    times = W_theta[pos].stack(flat=("freqs", "times")).times.data
    freqs = W_theta[pos].stack(flat=("freqs", "times")).freqs.data

    labeled_bursts = labeled_theta_bursts[pos].reshape(-1)

    unique_labels = np.unique(labeled_bursts)[1:]
    n_labels = len(unique_labels)

    S = np.zeros((n_labels, n_labels))
    F_min = np.zeros(n_labels)
    F_max = np.zeros(n_labels)
    F_mean = np.zeros(n_labels)

    for l_i in unique_labels - 1:
        F_min[l_i] = freqs[labeled_bursts == l_i + 1].min()
        F_max[l_i] = freqs[labeled_bursts == l_i + 1].max()
        F_mean[l_i] = freqs[labeled_bursts == l_i + 1].mean()

    for l_i in range(n_labels - 1):
        times_i = times[labeled_bursts == l_i + 1]
        s_i = len(times_i)
        for l_j in range(l_i, n_labels):
            if F_max[l_i] < F_min[l_j]:
                times_j = times[labeled_bursts == l_j + 1]
                s_j = len(times_j)
                S[l_i, l_j] = len(np.intersect1d(times_i, times_j)) / np.min((s_i, s_j))
    np.fill_diagonal(S, 0)
    SIZES += [S]
    FREQS += [F_mean]


x, y = [], []
for i in range(len(SIZES)):
    pi, pj = np.where(SIZES[i] > 0.95)
    x += [FREQS[i][pi]]
    y += [FREQS[i][pj]]

x = np.hstack(x)
y = np.hstack(y)
plt.hist2d(x, y, cmap="hot_r", density=True)
plt.xlabel("Frequency [Hz]")
plt.ylabel("Frequency [Hz]")
plt.title(r"Density of harmonic $\theta$ bursts")
plt.colorbar()


np.where(n_overlaps[:36] == 2)


plt.imshow(labeled_gamma_bursts[0] == 16, aspect="auto", cmap="binary_r")


composite.isel(bands=1, times=slice(6539, 6612), batches=0).plot()
composite.isel(bands=0, times=slice(6539, 6612), batches=0).plot()


W_gamma.times.data[(labeled_gamma_bursts[0] == 26).sum(0) > 0]



