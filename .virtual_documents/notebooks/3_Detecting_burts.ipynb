


import sys

sys.path.insert(1, "/home/vinicius/storage1/projects/vanderbilt")


import os

import emd
import igraph as ig
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import PyEMD
import scipy
import skimage as ski
import umap
import xarray as xr
from frites.utils import parallel_func
from mne.time_frequency import psd_array_multitaper
from scipy.optimize import curve_fit
from skimage.segmentation import watershed
from tqdm import tqdm

from config import metadata
from VUDA.emd import emd_vec
from VUDA.io.loadbinary import LoadBinary





from functools import partial

import jax
import jax.numpy as jnp

jax.config.update("jax_platform_name", "cpu")


def get_bc(cycles, freq, k_sd=5):
    return cycles / (k_sd * freq)


def cxmorelet(freq, cycles, sampling_freq):
    t = jnp.linspace(-1, 1, sampling_freq * 2)

    bc = get_bc(cycles, freq)
    norm = 1 / (bc * jnp.sqrt(2 * jnp.pi))
    gauss = jnp.exp(-(t**2) / (2 * bc**2))
    sine = jnp.exp(1j * 2 * jnp.pi * freq * t)

    wavelet = norm * gauss * sine
    return wavelet / jnp.sum(jnp.abs(wavelet))


@partial(jax.jit, static_argnums=3)
@partial(jax.vmap, in_axes=(None, 0, None, None))
def wavelet_transform(signal, freq, cycles, sampling_freq):
    wavelet = cxmorelet(freq, cycles, sampling_freq)
    return jax.scipy.signal.convolve(signal, wavelet, mode="same")


@partial(jax.jit, static_argnums=3)
@partial(jax.vmap, in_axes=(None, None, 0, None))
def superlet_transform_helper(signal, freqs, order, sampling_freq):
    return wavelet_transform(signal, freqs, order, sampling_freq) * jnp.sqrt(2)


def order_to_cycles(base_cycle, max_order, mode):
    if mode == "add":
        return jnp.arange(0, max_order) + base_cycle
    elif mode == "mul":
        return jnp.arange(1, max_order + 1) * base_cycle
    else:
        raise ValueError('mode should be one of "mul" or "add"')


def get_order(f, f_min: int, f_max: int, o_min: int, o_max: int):
    return o_min + jnp.round((o_max - o_min) * (f - f_min) / (f_max - f_min))


@partial(jax.vmap, in_axes=(0, None))
def get_mask(order, max_order):
    return jnp.arange(1, max_order + 1) > order


@jax.jit
def norm_geomean(X, root_pows, eps):
    X = jnp.log(X + eps).sum(axis=0)

    return jnp.exp(X / jnp.array(root_pows).reshape(-1, 1))


# @jax.jit
def adaptive_superlet_transform(
    signal,
    freqs,
    sampling_freq: int,
    base_cycle: int,
    min_order: int,
    max_order: int,
    eps=1e-12,
    mode="mul",
):
    """Computes the adaptive superlet transform of the provided signal
    Args:
        signal (jnp.ndarray): 1D array containing the signal data
        freqs (jnp.ndarray): 1D sorted array containing the frequencies to compute the wavelets at
        sampling_freq (int): Sampling frequency of the signal
        base_cycle (int): The number of cycles corresponding to order=1
        min_order (int): The minimum upper limit of orders to be used for a frequency in the adaptive superlet.
        max_order (int): The maximum upper limit of orders to be used for a frequency in the adaptive superlet.

        eps (float, optional): Epsilon value to be used for numerical stability in the geometric mean. Defaults to 1e-12.
        mode (str, optional): "add" or "mul", corresponding to the use of additive or multiplicative adaptive superlets. Defaults to "mul".
    Returns:
        jnp.ndarray: 2D array (Frequency x Time) representing the computed scalogram
    """
    cycles = order_to_cycles(base_cycle, max_order, mode)
    orders = get_order(freqs, min(freqs), max(freqs), min_order, max_order)
    mask = get_mask(orders, max_order)

    out = superlet_transform_helper(signal, freqs, cycles, sampling_freq)
    out = out.at[mask.T].set(1)

    return norm_geomean(out, orders, eps)


def superlets(
    data,
    freqs,
    sampling_freq,
    base_cycle=3,
    min_order=1,
    max_order=30,
    eps=1e-12,
    mode="mul",
):
    if not isinstance(data, jax.numpy.ndarray):
        data = jnp.array(data)
    if not isinstance(freqs, jax.numpy.ndarray):
        freqs = jnp.array(freqs)

    _fcn = partial(
        adaptive_superlet_transform,
        freqs=freqs,
        sampling_freq=sampling_freq,
        base_cycle=base_cycle,
        min_order=min_order,
        max_order=max_order,
        eps=eps,
        mode=mode,
    )

    def _loop(carry, array):
        return carry, _fcn(array)

    tf = jax.vmap(jax.vmap(_fcn, in_axes=(0,)), in_axes=(0,))

    return tf(data)


def jax_euclidean(matrix: np.ndarray) -> np.ndarray:
    """
    Calculate the element-wise absolute difference of each row in the matrix from the matrix itself.

    Parameters:
    - matrix (np.ndarray): Input matrix.

    Returns:
    - np.ndarray: Resulting matrix of absolute differences.
    """

    def _for_each(carry, row):
        return carry, jnp.abs((row - matrix))

    _, result = jax.lax.scan(_for_each, None, matrix)
    return result


def resolve_neighborhood(data: np.ndarray) -> np.ndarray:
    """
    Resolve neighborhood relationships in the input data based on a threshold.

    Parameters:
    - data (np.ndarray): Input thresholded data.
    Returns:
    - np.ndarray: Boolean array indicating neighborhood relationships.
    """
    data = jnp.array(data, dtype=np.float32)

    # Get index coordinates of values where the mask is true
    pairs = np.stack(np.where(data), axis=1)

    def _for_pair(carry, pair):
        x = jnp.abs(pair - pairs)
        # No difference in coordinate x_i - x_j can be greater than one
        # and at least one x_i has to be one for being a neighbor
        return carry, jnp.logical_and((x == 1).any(axis=1), ~(x > 1).any(axis=1))

    _, result = jax.lax.scan(_for_pair, None, pairs)

    return result, pairs


def return_labeled_blobs(data: np.ndarray, thr: float) -> np.ndarray:
    """
    Return labeled blobs in the input data based on neighborhood relationships.

    Parameters:
    - data (np.ndarray): Input data.
    - thr (float): Threshold value.

    Returns:
    - np.ndarray: Labeled blobs.
    """

    # Threshold data
    mask = data >= thr

    d, pairs = resolve_neighborhood(mask)

    G = nx.from_numpy_array(d)

    labels = np.zeros(d.shape[0], dtype=int)
    cc = sorted(nx.connected_components(G))

    for l, cc_ in enumerate(cc):
        idx = np.array(list(cc_))
        labels[idx] = l + 1

    # Create a labeled array
    labeled = np.array(mask, dtype=int)
    labeled[tuple(pairs.T)] = labels

    return labeled


def return_labeled_image(img: list, threshold: float):
    """
    Label regions in a binary image using a given threshold.

    Parameters:
    - img (numpy.ndarray): The binary image to label.
    - threshold (float): The threshold for labeling.

    Returns:
    - numpy.ndarray: A labeled image with connected regions.
    - numpy.ndarray: Array of unique labels.
    - int: The number of unique labels.

    This function labels connected regions in a binary image based on a given threshold.
    It uses the `ski.measure.label` function from the scikit-image library to perform
    the labeling. The resulting labeled image contains connected regions with unique
    labels, and the number of labels is also returned.

    Example:
    labeld_image, labels, nlabels = return_labeled_image(binary_img, 0.5)
    """
    # Labeled image
    labeld_image = ski.measure.label(img > threshold, background=0)
    # Get unique labels
    labels = np.unique(labeld_image)[1:]
    # Number of labels
    nlabels = len(labels)

    return labeld_image, labels, nlabels


def detect_burts(
    spectra: xr.DataArray,
    init_threshold: float,
    min_threshold: float,
    gamma: float,
    relative: bool = True,
):
    """
    Detect bursts in spectra using a dynamic thresholding approach.

    Parameters:
    - spectra (numpy.ndarray): The input spectra to analyze.
    - init_threshold (float): The initial threshold for labeling.
    - min_threshold (float): The minimum threshold to stop the labeling process.

    Returns:
    - numpy.ndarray: An image with labeled bursts.

    This function detects bursts in a given spectra by iteratively updating a labeling
    based on threshold values. It starts with an initial threshold, and in each iteration,
    it updates the labeling using a lower threshold. The process continues until the
    threshold reaches the minimum threshold value. The resulting labeled image contains
    burst regions.

    Note: The input spectra are first z-scored before applying labeling.

    Example:
    labeled_bursts = detect_burts(spectra_data, 2.0, 1.0)
    """
    # Dimensions of the spectra
    size = spectra.shape
    # z-score spectra
    if relative:
        z = (spectra - spectra.mean("times")) / spectra.std("times")
    else:
        z = (spectra - spectra.mean()) / spectra.std()
    # label image using initial threshold
    labeled_image, labels, nlabels = return_labeled_image(z, init_threshold)

    # Update threshold
    thr = init_threshold - gamma

    while thr >= min_threshold:

        # Label image for new threshold
        new_labeled_image, new_labels, new_nlabels = return_labeled_image(z, thr)
        # Work with the flattened matrix
        new_labeled_image = new_labeled_image.reshape(-1)

        # Copy original image
        temp = labeled_image.copy().reshape(-1)
        # Get biggest label
        max_label = labels.max()

        # Check mergings of burts
        for nl in new_labels:
            # For a given label in the new labeled image
            index_nl = np.where(new_labeled_image == nl)[0]
            # Check if in the previous one it corresponded to two or more burts
            if len(np.unique(temp[index_nl])) > 2:
                # If yes, keep old labeling
                new_labeled_image[index_nl] = temp[index_nl]

        # Check if new labels contain old ones
        for nl in new_labels:
            # For a given label in the new labeled image
            intersection = []
            index_nl = np.where(new_labeled_image == nl)[0]
            # For a given label in the old labeled image
            for l in labels:
                # Keep the indexes where new-contains-old labels
                index_l = np.where(labeled_image == l)[0]
                if len(np.isin(index_l, index_nl)):
                    intersection.append(l)

            # Update region of intersect in case any was found
            if len(intersection) > 0:
                intersection = np.hstack(intersection)
                indexes = np.where(np.intersect1d(temp, intersection))
                temp[index_nl] = intersection.max()
            else:
                max_label = max_label + 1
                temp[index_nl] = max_label

        # Update labeled image
        labeled_image, labels, nlabels = new_labeled_image, new_labels, new_nlabels
        # Update threshold
        thr = thr - gamma

    # Reset labels in the final image
    labeled_image, labels, nlabels = return_labeled_image(
        labeled_image.reshape(size), 0
    )

    return labeled_image





date = "10-20-2022"
monkey = "FN"
max_imfs = None
method = "eemd"


composites_path = os.path.expanduser(
    f"~/funcog/HoffmanData/{monkey}/{date}/composite_signals_task_method_eemd_max_imfs_None.nc"
)

ps_composites_path = os.path.expanduser(
    f"~/funcog/HoffmanData/{monkey}/{date}/ps_composite_signals_task_method_eemd_max_imfs_None.nc"
)


composites = xr.open_dataset(composites_path)
ps_composites = xr.open_dataset(ps_composites_path)


channel = "channel33"

data = ps_composites[channel].dropna("IMFs")

freqs = data.freqs.data
kernel = np.hanning(50)

data_sm = xr.DataArray(
    scipy.signal.fftconvolve(data, kernel[None, None, :], mode="same", axes=2),
    dims=data.dims,
    coords=data.coords,
)

freqs = data.freqs.data

n_blocks, n_IMFs, n_freqs = data.shape

peaks = freqs[data_sm.argmax("freqs")]


slow_idx = np.logical_and(peaks.flatten() >= 0, peaks.flatten() <= 10).reshape(
    peaks.shape
)

fast_idx = np.logical_and(peaks.flatten() >= 55, peaks.flatten() <= np.inf).reshape(
    peaks.shape
)


x = composites[channel].dropna("IMFs")


slow = (x * slow_idx[..., None]).sum("IMFs")
fast = (x * fast_idx[..., None]).sum("IMFs")
filtered = xr.concat((slow, fast), "freqs")


f_mt = partial(
    psd_array_multitaper,
    fmin=0,
    fmax=300,
    sfreq=1000,
    verbose=True,
    bandwidth=4,
    n_jobs=20,
)


psd, f = f_mt(filtered)


plt.semilogx(f, psd[0].T / 6e6, c="b", lw=0.2)
plt.semilogx(f, psd[1].T / 2500, c="g", lw=0.2);


channels = list(composites.keys())


X = []

for channel in tqdm(channels):
    data = ps_composites[channel].load().dropna("IMFs")
    x = composites[channel].load().dropna("IMFs")

    freqs = data.freqs.data
    kernel = np.hanning(50)

    data_sm = xr.DataArray(
        scipy.signal.fftconvolve(data, kernel[None, None, :], mode="same", axes=2),
        dims=data.dims,
        coords=data.coords,
    )

    freqs = data.freqs.data

    n_blocks, n_IMFs, n_freqs = data.shape

    peaks = freqs[data_sm.argmax("freqs")]

    slow_idx = np.logical_and(peaks.flatten() >= 0, peaks.flatten() <= 10).reshape(
        peaks.shape
    )

    fast_idx = np.logical_and(peaks.flatten() >= 55, peaks.flatten() <= np.inf).reshape(
        peaks.shape
    )

    slow = (x * slow_idx[..., None]).sum("IMFs")
    fast = (x * fast_idx[..., None]).sum("IMFs")
    X += [xr.concat((slow, fast), "components")]


X = xr.concat(X, "channels")


freqs = np.linspace(3, 150, 50)

W = []
for i in tqdm(range(40)):
    w = adaptive_superlet_transform(
        X.sel(blocks=100, components=1, channels=i).data,
        freqs,
        sampling_freq=1000,
        base_cycle=3,
        min_order=1,
        max_order=30,
        eps=1e-12,
        mode="mul",
    )

    W += [np.real(w * np.conj(w))]


W = xr.DataArray(
    np.stack(W), dims=("channels", "freqs", "times"), coords={"freqs": freqs}
)


sxx = (W - W.mean(("times", "freqs"))) / W.std(("times", "freqs"))


plt.imshow(
    W[33] ,
    aspect="auto",
    cmap="turbo",
    vmax=100,
    origin="lower",
)
plt.yticks(np.arange(len(freqs))[::5], np.round(freqs[::5], 0))
plt.colorbar()
plt.ylabel("frequency [Hz]")
plt.xlabel("time")


%timeit return_labeled_blobs(sxx.isel(times=slice(10000, 12000)).data, 3)


nonzero_coords = np.transpose(np.nonzero(sxx.isel(times=slice(10000, 15000)).data >= 3))

# Visualize the 3D matrix
fig = plt.figure(figsize=(3, 3), dpi=300)
ax = fig.add_subplot(111, projection="3d")
plt.title("Two blobs")

# Scatter plot for each non-zero element
ax.scatter(
    nonzero_coords[:, 2], nonzero_coords[:, 1], nonzero_coords[:, 0], c="b", marker="o"
)

ax.set_xlabel("Time")
ax.set_ylabel("Frequency")
ax.set_zlabel("Channels", labelpad=-5)
ax.view_init(elev=0, azim=45)


# Create a 3D scatter plot
fig = plt.figure(figsize=(3, 3), dpi=300)
ax = fig.add_subplot(111, projection="3d")
cmap = plt.get_cmap("turbo")
for l in range(1, labeled.max() + 1):
    x, y, z = np.where(labeled == l)
    color = cmap(l / labeled.max())
    ax.scatter(z, y, x, marker=".", label=f"l={l}", color=color)
ax.set_xlabel("Time")
ax.set_ylabel("Frequency")
ax.set_zlabel("Channels", labelpad=-5)
ax.view_init(elev=0, azim=45)


plt.imshow(labeled.max(1), aspect="auto", cmap="turbo", origin="lower")





date = "10-20-2022"
monkey = "FN"


filepath = os.path.expanduser(f"~/funcog/HoffmanData/{monkey}/{date}/aHPC_B_cnct.nc")


data = xr.load_dataarray(filepath)


# Get zero timestamp
t_init = data.times.data[0]
# Get final timestamp
t_end = data.times.data[0]
# End of treehouse
t_th_end = float(data.attrs["TH_end"].split(", ")[1])
# Beggining of sleep
t_sleep_init = float(data.attrs["Sleep_start"].split(", ")[1])


data = data.sel(times=slice(t_init, t_th_end))


times = data.times.values


plt.figure(figsize=(20, 4))
ax = plt.subplot(111)
data.sel(channels=33).plot()
[ax.spines[key].set_visible(False) for key in ["top", "right"]];





def standardize_imf_per_block(IMFs):
    """
    Standardizes the number of intrinsic mode functions (IMFs) per block in an xarray Dataset.
    It sums slower IMFs in case a given block has more IMFs, thant the block that has the least
    number of IMFs.

    Parameters:
    - IMFs (xarray.Dataset): Input Dataset containing IMFs with dimensions ('blocks', 'IMFs', ...).

    Returns:
    - xarray.Dataset: Output Dataset with standardized IMFs per block.

    The function standardizes the number of IMFs per block by either summing the first
    (10 - n_imfs_min + 1) IMFs or keeping the original IMFs if the number is already less
    than or equal to n_imfs_min.

    Note:
    - The function assumes that the input Dataset has dimensions ('blocks', 'IMFs', ...).
    - The result is a new Dataset with standardized IMFs per block.
    """

    assert isinstance(IMFs, xr.DataArray)
    np.testing.assert_array_equal(IMFs.dims, ("blocks", "IMFs", "times"))

    n_imfs_min = IMFs.n_imfs_per_block.min()
    attrs = IMFs.attrs

    reduced = []

    for i in range(IMFs.sizes["blocks"]):

        temp = IMFs[i].dropna("IMFs").drop_vars("IMFs")
        n_imfs = temp.shape[0]

        if n_imfs > n_imfs_min:

            reduced += [
                xr.concat(
                    (
                        temp[0 : n_imfs - n_imfs_min + 1].sum("IMFs", keepdims=True),
                        temp[n_imfs - n_imfs_min + 1 :],
                    ),
                    "IMFs",
                )
            ]

        else:
            reduced += [temp]

    IMFs = xr.concat(reduced, "blocks")
    IMFs.attrs = attrs

    return IMFs


IMFs_single = emd_vec(
    data.sel(channels=33).values,
    times,
    method="eemd",
    max_imfs=None,
    block_size=200,
    nensembles=5,
    use_min_block_size=True,
    remove_fastest_imf=True,
    n_jobs=20,
    imf_opts={"stop_method": "fixed", "max_iters": 5},
)


IMFs_single = standardize_imf_per_block(IMFs_single)


imf = IMFs_single[0].data

z_imf = (imf - imf.mean(1)[:, None]) / imf.std(1)[:, None]


fig, axd = plt.subplot_mosaic(
    [["A", "A", "C", "C"], ["B", "B", "C", "C"], ["B", "B", "C", "C"]],
    layout="constrained",
    figsize=(10, 5),
    dpi=600,
)

# Plot the signal snippet
plt.sca(axd["A"])
plt.plot(data.sel(channels=33).data)
plt.xlim(0, 5000)
plt.xlabel("Time [a.u]")
plt.ylabel(r"LFP [$\mu$V]")
[axd["A"].spines[key].set_visible(False) for key in ["top", "right"]]

# Plot the signal IMFs
plt.sca(axd["B"])
colors = []
for i in range(imf.shape[0]):
    p_ = plt.plot(z_imf[i] + 14.5 * (z_imf.shape[0] - i))
    colors += [p_[0].get_color()]
    plt.text(
        -490,
        14.5 * (z_imf.shape[0] - i),
        f"IMF {i + 1}",
        color=colors[-1],
        rotation=20,
    )
    plt.axis("off")


psd, f = psd_array_multitaper(
    imf,
    fmin=0,
    fmax=300,
    sfreq=1000,
    bandwidth=4,
    verbose=False,
    n_jobs=10,
)

plt.sca(axd["C"])
for i in range(len(psd)):
    plt.semilogx(f, psd[i] / psd[i].sum())
plt.xlabel("frequency [Hz]")
plt.ylabel("Norm. PSD")
[axd["C"].spines[key].set_visible(False) for key in ["top", "right"]];





IMFs = IMFs_single.stack(samples=("blocks", "IMFs")).T.dropna("samples")


SXX, f = psd_array_multitaper(
    IMFs,
    fmin=0,
    fmax=300,
    sfreq=1000,
    verbose=True,
    bandwidth=4,
    n_jobs=20,
)

SXX_norm = SXX / SXX.mean(1)[:, None]


reducer = umap.UMAP(
    n_jobs=20,
    min_dist=0.5,
    n_neighbors=5,
)
embedding = reducer.fit_transform(SXX_norm)


from sklearn.cluster import DBSCAN, OPTICS, KMeans

clustering = KMeans(n_clusters=6, init="k-means++", n_init="auto").fit(SXX_norm)

clustering = DBSCAN(eps=200, min_samples=20).fit(SXX_norm)


clustering.labels_


plt.scatter(embedding[:, 0], embedding[:, 1], s=0.5, c=clustering.labels_, cmap="tab10")


unique_labels = np.unique(clustering.labels_)


n_cluster = len(unique_labels)


from functools import partial


def average_for_cluster(data=None, cluster_labels=None, label=None):
    return data[cluster_labels == label].mean(0)


partial_average_for_cluster = partial(
    average_for_cluster, data=SXX_norm, cluster_labels=clustering.labels_
)

out = np.stack([partial_average_for_cluster(label=label) for label in unique_labels])

order = np.argsort(out.argmax(axis=1))


unique_labels[order]


plt.figure(figsize=(10, 5))
for pos, i in enumerate(unique_labels[order]):
    plt.subplot(3, 3, pos + 1)
    plt.semilogx(f, SXX_norm[clustering.labels_ == i].T, lw=0.1, color="b")
    plt.semilogx(f, SXX_norm[clustering.labels_ == i].mean(0), lw=2, color="k")
    plt.xlabel("Frequency [Hz]")
    plt.ylabel("Norm. POWER")
plt.tight_layout()


n_blocks = IMFs_single.shape[0]
n_imfs_per_block = IMFs_single.n_imfs_per_block


# win = np.concatenate( ([0], np.cumsum(n_imfs_per_block) ) )


cluster_labels = clustering.labels_.reshape(-1, 10)


composite = []

for i in range(n_blocks):
    # labels = clustering.labels_[win[i] : win[i + 1]]
    labels = cluster_labels[i]
    composite += [
        IMFs_single[i]
        .dropna("IMFs")
        .assign_coords({"IMFs": labels})
        .groupby("IMFs")
        .sum("IMFs")
    ]


composite = xr.concat(composite, "blocks")


SXX_composite, f = psd_array_multitaper(
    composite,
    fmin=0,
    fmax=300,
    sfreq=1000,
    verbose=True,
    bandwidth=4,
    n_jobs=20,
)

SXX_composite_norm = SXX_composite / SXX_composite.mean(1)[:, None]


plt.figure(figsize=(10, 5))
for pos, i in enumerate(unique_labels[order]):
    plt.subplot(3, 3, pos + 1)
    plt.semilogx(f, SXX_composite_norm[:, i, :].T, c="b", lw=0.1)
    plt.semilogx(f, np.nanmean(SXX_composite_norm[:, i, :], 0), c="r", lw=3);

















def mt_decomposition(
    data, sfreq, fmin=0, fmax=np.inf, bandwidth=4, verbose=False, n_jobs=1
):
    """
    Perform multitaper decomposition on input data.

    Parameters:
    - data (numpy.ndarray): Input data with shape (n_channels, n_blocks, n_imfs, n_times).
    - sfreq (float): Sampling frequency of the data.
    - fmin (float, optional): Minimum frequency for PSD computation (default: 0).
    - fmax (float, optional): Maximum frequency for PSD computation (default: np.inf).
    - bandwidth (int, optional): Bandwidth parameter for multitaper decomposition (default: 4).
    - verbose (bool, optional): Flag to enable/disable verbose output (default: False).
    - n_jobs (int, optional): Number of parallel jobs to run (default: 1).

    Returns:
    - xr.DataArray: Multitaper PSD results with dimensions ('channels', 'blocks', 'IMF', 'freqs').
                   The 'freqs' coordinate represents the mean frequency vector.

    Note:
    This function uses parallel processing to compute the power spectral density (PSD)
    for each sample in the input data. The results are stacked to form a DataArray.

    Example:
    ```python
    data = ...  # provide input data
    sfreq = 1000  # set sampling frequency
    result = mt_decomposition(data, sfreq)
    print(result)
    ```

    """

    n_blocks, n_imfs, n_times = data.shape

    def _for_block(i):
        return psd_array_multitaper(
            data[i],
            fmin=fmin,
            fmax=fmax,
            sfreq=sfreq,
            verbose=False,
            bandwidth=bandwidth,
            n_jobs=1,
        )

    # define the function to compute in parallel
    parallel, p_fun = parallel_func(
        _for_sample, verbose=verbose, n_jobs=n_jobs, total=n_samples
    )

    out = parallel(p_fun(i) for i in range(n_samples))

    fvec = np.stack([out[i][1] for i in range(n_samples)]).mean(0)

    SXX = np.stack([out[i][0] for i in range(n_samples)]).reshape(
        n_channels, n_blocks, n_imfs, len(fvec)
    )

    SXX = xr.DataArray(
        SXX, dims=("channels", "blocks", "IMF", "freqs"), coords={"freqs": fvec}
    )

    return SXX


out = mt_decomposition(IMFs_single, 1000, fmin=0, fmax=300, bandwidth=2, n_jobs=40)


idx = np.random.choice(out.channels.values, 10, replace=False)


SXX = out.sel(channels=idx).stack(IMFs=("channels", "blocks", "IMF")).T.values


SXX_norm = SXX / SXX.sum(1)[:, None]


reducer = umap.UMAP(n_jobs=20, min_dist=0.5, n_neighbors=5)
embedding = reducer.fit_transform(SXX_norm)





from fooof import FOOOFGroup

fg = FOOOFGroup(max_n_peaks=1)
fg.fit(f, SXX_norm, [f.min(), f.max()])


colors = []
for i in range(len(fg.get_results())):
    try:
        colors += [fg.get_results()[i].peak_params.squeeze()[0]]
    except:
        colors += [0]


import numpy as np
from sklearn.cluster import KMeans

clustering = KMeans(n_clusters=4, n_init="auto")
cluster_labels = clustering.fit_predict(SXX_norm)


plt.scatter(embedding[:, 0], embedding[:, 1], s=0.1, c=cluster_labels, cmap="tab10")


from sklearn.cluster import DBSCAN

clustering = DBSCAN(eps=0.03, min_samples=20, n_jobs=20)

cluster_labels = clustering.fit_predict(SXX_norm)


plt.scatter(embedding[:, 0], embedding[:, 1], s=0.1, c=cluster_labels, cmap="tab10")


import numpy as np
from sklearn.cluster import AffinityPropagation

X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])
clustering = AffinityPropagation(damping=0.5)
cluster_labels = clustering.fit_predict(SXX_norm)


plt.scatter(embedding[:, 0], embedding[:, 1], s=0.1, c=cluster_labels, cmap="tab10")


cluster_labels += 1


unique_labels = np.unique(cluster_labels)
n_clusters = len(unique_labels)
print(n_clusters, unique_labels)














order


titles = ["SLOW 1", "SLOW 2", "SLOW 3", "MEDIUM 1", "MEDIUM 2", "FAST"]


print(unique_labels[order])


plt.figure(figsize=(12, 5))
for pos, i in enumerate(unique_labels[order]):
    plt.subplot(3, 3, pos + 1)
    #  plt.semilogx(f, SXX_norm[cluster_labels == i].T, lw=0.1, color="b")
    plt.semilogx(f, SXX_norm[cluster_labels == i].mean(0), lw=2, color="k")
    plt.xlabel("Frequency [Hz]")
    plt.ylabel("Norm. POWER")
    # plt.title(titles[pos])
plt.tight_layout()








from functools import partial

import jax
import jax.numpy as jnp


def get_bc(cycles, freq, k_sd=5):
    return cycles / (k_sd * freq)


def cxmorelet(freq, cycles, sampling_freq):
    t = jnp.linspace(-1, 1, sampling_freq * 2)

    bc = get_bc(cycles, freq)
    norm = 1 / (bc * jnp.sqrt(2 * jnp.pi))
    gauss = jnp.exp(-(t**2) / (2 * bc**2))
    sine = jnp.exp(1j * 2 * jnp.pi * freq * t)

    wavelet = norm * gauss * sine
    return wavelet / jnp.sum(jnp.abs(wavelet))


@partial(jax.jit, static_argnums=3)
@partial(jax.vmap, in_axes=(None, 0, None, None))
def wavelet_transform(signal, freq, cycles, sampling_freq):
    wavelet = cxmorelet(freq, cycles, sampling_freq)
    return jax.scipy.signal.convolve(signal, wavelet, mode="same")


@partial(jax.jit, static_argnums=3)
@partial(jax.vmap, in_axes=(None, None, 0, None))
def superlet_transform_helper(signal, freqs, order, sampling_freq):
    return wavelet_transform(signal, freqs, order, sampling_freq) * jnp.sqrt(2)


def order_to_cycles(base_cycle, max_order, mode):
    if mode == "add":
        return jnp.arange(0, max_order) + base_cycle
    elif mode == "mul":
        return jnp.arange(1, max_order + 1) * base_cycle
    else:
        raise ValueError('mode should be one of "mul" or "add"')


def get_order(f, f_min: int, f_max: int, o_min: int, o_max: int):
    return o_min + jnp.round((o_max - o_min) * (f - f_min) / (f_max - f_min))


@partial(jax.vmap, in_axes=(0, None))
def get_mask(order, max_order):
    return jnp.arange(1, max_order + 1) > order


@jax.jit
def norm_geomean(X, root_pows, eps):
    X = jnp.log(X + eps).sum(axis=0)

    return jnp.exp(X / jnp.array(root_pows).reshape(-1, 1))


# @jax.jit
def adaptive_superlet_transform(
    signal,
    freqs,
    sampling_freq: int,
    base_cycle: int,
    min_order: int,
    max_order: int,
    eps=1e-12,
    mode="mul",
):
    """Computes the adaptive superlet transform of the provided signal
    Args:
        signal (jnp.ndarray): 1D array containing the signal data
        freqs (jnp.ndarray): 1D sorted array containing the frequencies to compute the wavelets at
        sampling_freq (int): Sampling frequency of the signal
        base_cycle (int): The number of cycles corresponding to order=1
        min_order (int): The minimum upper limit of orders to be used for a frequency in the adaptive superlet.
        max_order (int): The maximum upper limit of orders to be used for a frequency in the adaptive superlet.

        eps (float, optional): Epsilon value to be used for numerical stability in the geometric mean. Defaults to 1e-12.
        mode (str, optional): "add" or "mul", corresponding to the use of additive or multiplicative adaptive superlets. Defaults to "mul".
    Returns:
        jnp.ndarray: 2D array (Frequency x Time) representing the computed scalogram
    """
    cycles = order_to_cycles(base_cycle, max_order, mode)
    orders = get_order(freqs, min(freqs), max(freqs), min_order, max_order)
    mask = get_mask(orders, max_order)

    out = superlet_transform_helper(signal, freqs, cycles, sampling_freq)
    out = out.at[mask.T].set(1)

    return norm_geomean(out, orders, eps)


def superlets(
    data,
    freqs,
    sampling_freq,
    base_cycle=3,
    min_order=1,
    max_order=30,
    eps=1e-12,
    mode="mul",
):
    if not isinstance(data, jax.numpy.ndarray):
        data = jnp.array(data)
    if not isinstance(freqs, jax.numpy.ndarray):
        freqs = jnp.array(freqs)

    _fcn = partial(
        adaptive_superlet_transform,
        freqs=freqs,
        sampling_freq=sampling_freq,
        base_cycle=base_cycle,
        min_order=min_order,
        max_order=max_order,
        eps=eps,
        mode=mode,
    )
    tf = jax.vmap(jax.vmap(_fcn, in_axes=(0,)), in_axes=(0,))
    return tf(data)


freqs = np.arange(1, 20, 1)

slow1 = IMFs_single[0][cluster_labels[:9] == 5].mean(0)
slow2 = IMFs_single[0][cluster_labels[:9] == 4].mean(0)

slow1 = (slow1 - slow1.mean()) / slow1.std()
slow2 = (slow2 - slow2.mean()) / slow2.std()

out1 = adaptive_superlet_transform(
    slow1,
    freqs,
    sampling_freq=1000,
    base_cycle=3,
    min_order=1,
    max_order=20,
    eps=1e-12,
    mode="mul",
)

out2 = adaptive_superlet_transform(
    slow2 + slow1,
    freqs,
    sampling_freq=1000,
    base_cycle=3,
    min_order=1,
    max_order=30,
    eps=1e-12,
    mode="mul",
)


plt.imshow(
    (out1 * np.conj(out1)).real,
    aspect="auto",
    cmap="turbo",
    origin="lower",
    vmin=0,
    vmax=0.6,
)

plt.plot(slow1 + 7, color="w", lw=1)
plt.yticks(np.arange(len(freqs)), np.round(freqs, 2))
plt.ylabel("Frequency [Hz]")
plt.xlabel("Time")
plt.colorbar()
plt.title("SLOW 1")


plt.imshow(
    (out2 * np.conj(out2)).real,
    aspect="auto",
    cmap="turbo",
    origin="lower",
    vmin=0,
    vmax=1,
)

plt.plot(slow2 + 7, color="w")
plt.yticks(range(len(freqs)), np.round(freqs, 2))
plt.ylabel("Frequency [Hz]")
plt.xlabel("Time")
plt.colorbar()
plt.title("SLOW1 + SLOW 2")


out3 = adaptive_superlet_transform(
    data.sel(channels=33)[:9774].data,
    freqs,
    sampling_freq=1000,
    base_cycle=3,
    min_order=1,
    max_order=30,
    eps=1e-12,
    mode="mul",
)


plt.imshow(
    (out3 * np.conj(out3)).real,
    aspect="auto",
    cmap="turbo",
    origin="lower",
    vmin=0,
    vmax=3000,
)
plt.colorbar()
plt.yticks(range(len(freqs)), np.round(freqs, 2))
plt.ylabel("Frequency [Hz]")
plt.xlabel("Time")


mt1, fvec1 = psd_array_multitaper(
    slow1, 1000, fmin=0, bandwidth=4, fmax=300, output="power"
)
mt2, fvec2 = psd_array_multitaper(
    slow2, 1000, fmin=0, bandwidth=4, fmax=300, output="power"
)
plt.semilogx(fvec1, mt1)
plt.semilogx(fvec2, mt2)


plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.scatter(x=embedding[:, 0], y=embedding[:, 1], c=colors, cmap="jet", s=3)
cbar = plt.colorbar()
cbar.ax.set_ylabel("Frequency [Hz]", rotation=270, labelpad=15)

plt.subplot(1, 2, 2)
plt.scatter(x=embedding[:, 0], y=embedding[:, 1], c=cluster_labels, cmap="jet", s=3)
cbar = plt.colorbar()
cbar.ax.set_ylabel("KMEANS labels", rotation=270, labelpad=15)





from scipy.signal import find_peaks


find_peaks(SXX[0], prominence=1)





plt.plot(SXX.freqs, SXX[1])


A = np.corrcoef(IMFs_single[0])
np.fill_diagonal(A, 0)
plt.imshow(A, vmin=-0.1, vmax=0.1, cmap="RdBu_r")
plt.colorbar()








x = data.sel(channels=33).squeeze().data[:14160]

_emd = PyEMD.EEMD(trials=50)
_emd.noise_seed(1234546)

imf = _emd.eemd(x, progress=False, max_imf=None)

z_imf = (imf - imf.mean(1)[:, None]) / imf.std(1)[:, None]


fig, axd = plt.subplot_mosaic(
    [["A", "A", "C", "C"], ["B", "B", "C", "C"], ["B", "B", "C", "C"]],
    layout="constrained",
    figsize=(10, 5),
    dpi=600,
)

# Plot the signal snippet
plt.sca(axd["A"])
plt.plot(x)
plt.xlim(0, 5000)
plt.xlabel("Time [a.u]")
plt.ylabel(r"LFP [$\mu$V]")
[axd["A"].spines[key].set_visible(False) for key in ["top", "right"]]

# Plot the signal IMFs
plt.sca(axd["B"])
colors = []
for i in range(imf.shape[0]):
    p_ = plt.plot(z_imf[i] + 14.5 * (z_imf.shape[0] - i))
    colors += [p_[0].get_color()]
    plt.text(
        -490,
        14.5 * (z_imf.shape[0] - i),
        f"IMF {i + 1}",
        color=colors[-1],
        rotation=20,
    )
    plt.axis("off")


psd, f = psd_array_multitaper(
    imf,
    fmin=0,
    fmax=300,
    sfreq=1000,
    verbose=False,
    bandwidth=12,
    n_jobs=10,
)

plt.sca(axd["C"])
for i in range(len(psd)):
    plt.semilogx(f, psd[i] / psd[i].sum())
plt.xlabel("frequency [Hz]")
plt.ylabel("Norm. PSD")
[axd["C"].spines[key].set_visible(False) for key in ["top", "right"]]

plt.savefig("figures/emf_snippet_example.png")





def gauss(x, a, x0, sigma):
    return a * np.exp(-((x - x0) ** 2) / (2 * sigma**2))


def gaussian_fit(y, x):
    try:
        popt, pcov = curve_fit(gauss, x, y, p0=[1, x[y.argmax()], 1])
    except:
        popt = np.nan
    return popt


"""
theta_composite = []
gamma_composite = []

for i in tqdm(range(1)):

    psd, f = psd_array_multitaper(
        IMFs[i], fmin=0, fmax=300, sfreq=1000, verbose=False, bandwidth=4, n_jobs=30
    )

    # Fit a gaussian to each spectrum and get their first momentum
    popt = np.apply_along_axis(gaussian_fit, 1, psd, f)
    mu = popt[:, 1]

    # theta composite
    theta_peaks = np.logical_and(mu >= 3, mu < 8)
    idx = np.where(theta_peaks)[0]
    theta_composite.append(IMFs[i][idx].sum(0))
    # gamma composite
    gamma_peaks = np.logical_and(mu >= 40, mu < 150)
    idx = np.where(gamma_peaks)[0]
    gamma_composite.append(IMFs[i][idx].sum(0))
""";


def _for_batch(IMF: list, bands: list, band_names: list = None):

    n_imfs, n_times = IMF.shape
    n_bands = len(bands)

    if not isinstance(band_names, (list, np.ndarray)):
        band_names = np.median(bands, axis=1).astype(int)

    psd, f = psd_array_multitaper(
        IMF, fmin=0, fmax=300, sfreq=1000, verbose=False, bandwidth=4, n_jobs=1
    )

    # Fit a gaussian to each spectrum and get their first momentum
    popt = np.apply_along_axis(gaussian_fit, 1, psd, f)
    mu = popt[:, 1]

    # Composite signals
    composite = np.empty((n_bands, n_times))

    for pos, (f_l, f_h) in enumerate(bands):
        peaks = np.logical_and(mu >= f_l, mu < f_h)
        idx = np.where(peaks)[0]
        composite[pos] = IMF[idx].sum(0)

    dims = ("bands", "times")
    coords = {"bands": band_names}

    composite = xr.DataArray(composite, dims=dims, coords=coords)

    return composite


# define the function to compute in parallel
parallel, p_fun = parallel_func(_for_batch, n_jobs=30, verbose=False, total=len(IMFs))
# Compute the single trial coherence
composite = parallel(p_fun(IMF, [[3, 8], [40, 150]]) for IMF in IMFs)


lengths = np.asarray([signal.shape[1] for signal in composite])


composite = xr.concat([signal[:, 0 : lengths.min()] for signal in composite], "batches")


plt.figure(figsize=(8, 4))
ax = plt.subplot(111)
plt.plot(np.squeeze(composite[100][1]), "k", label=r"$\gamma$")
plt.plot(np.squeeze(composite[100][0]), "r", lw=3, label=r"$\theta$")
plt.legend()
[ax.spines[key].set_visible(False) for key in ["top", "right"]]
plt.xlim(2500, 4500)
plt.xlabel("Time")





freqs = np.linspace(3, 10, 50)

W_theta = tfr_array_morlet(
    composite.isel(bands=[0]),
    1000,
    freqs,
    n_cycles=12,
    n_jobs=30,
).squeeze()

dims = ("batches", "freqs", "times")
coords = dict(freqs=freqs)
W_theta = xr.DataArray((W_theta * W_theta.conj()).real, dims=dims, coords=coords)


freqs = np.linspace(40, 150, 50)

W_gamma = tfr_array_morlet(
    composite.isel(bands=[1]),
    1000,
    freqs,
    n_cycles=freqs / 2,
    n_jobs=30,
).squeeze()

dims = ("batches", "freqs", "times")
coords = dict(freqs=freqs)
W_gamma = xr.DataArray((W_gamma * W_gamma.conj()).real, dims=dims, coords=coords)


x = composite.isel(bands=0, batches=0)
y = composite.isel(bands=1, batches=0)

x = (x - x.mean("times")) / (7 * x.std("times"))
y = (y - y.mean("times")) / (y.std("times"))


plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
W_theta[0].plot(cmap="turbo", vmax=1e6)
plt.plot(x + 9, "w")
plt.title(r"$\theta$-composite")
plt.xlim(5000, 8200)

plt.subplot(1, 2, 2)
W_gamma[0].plot(cmap="turbo", vmax=120000)
plt.plot(y + 133, "w")
plt.title(r"$\gamma$-composite")
plt.xlim(2500, 5000)





def return_labeled_image(img: list, threshold: float):
    """
    Label regions in a binary image using a given threshold.

    Parameters:
    - img (numpy.ndarray): The binary image to label.
    - threshold (float): The threshold for labeling.

    Returns:
    - numpy.ndarray: A labeled image with connected regions.
    - numpy.ndarray: Array of unique labels.
    - int: The number of unique labels.

    This function labels connected regions in a binary image based on a given threshold.
    It uses the `ski.measure.label` function from the scikit-image library to perform
    the labeling. The resulting labeled image contains connected regions with unique
    labels, and the number of labels is also returned.

    Example:
    labeld_image, labels, nlabels = return_labeled_image(binary_img, 0.5)
    """
    # Labeled image
    labeld_image = ski.measure.label(img > threshold, background=0)
    # Get unique labels
    labels = np.unique(labeld_image)[1:]
    # Number of labels
    nlabels = len(labels)

    return labeld_image, labels, nlabels


def detect_burts(
    spectra: xr.DataArray,
    init_threshold: float,
    min_threshold: float,
    gamma: float,
    relative: bool = True,
):
    """
    Detect bursts in spectra using a dynamic thresholding approach.

    Parameters:
    - spectra (numpy.ndarray): The input spectra to analyze.
    - init_threshold (float): The initial threshold for labeling.
    - min_threshold (float): The minimum threshold to stop the labeling process.

    Returns:
    - numpy.ndarray: An image with labeled bursts.

    This function detects bursts in a given spectra by iteratively updating a labeling
    based on threshold values. It starts with an initial threshold, and in each iteration,
    it updates the labeling using a lower threshold. The process continues until the
    threshold reaches the minimum threshold value. The resulting labeled image contains
    burst regions.

    Note: The input spectra are first z-scored before applying labeling.

    Example:
    labeled_bursts = detect_burts(spectra_data, 2.0, 1.0)
    """
    # Dimensions of the spectra
    size = spectra.shape
    # z-score spectra
    if relative:
        z = (spectra - spectra.mean("times")) / spectra.std("times")
    else:
        z = (spectra - spectra.mean()) / spectra.std()
    # label image using initial threshold
    labeled_image, labels, nlabels = return_labeled_image(z, init_threshold)

    # Update threshold
    thr = init_threshold - gamma

    while thr >= min_threshold:

        # Label image for new threshold
        new_labeled_image, new_labels, new_nlabels = return_labeled_image(z, thr)
        # Work with the flattened matrix
        new_labeled_image = new_labeled_image.reshape(-1)

        # Copy original image
        temp = labeled_image.copy().reshape(-1)
        # Get biggest label
        max_label = labels.max()

        # Check mergings of burts
        for nl in new_labels:
            # For a given label in the new labeled image
            index_nl = np.where(new_labeled_image == nl)[0]
            # Check if in the previous one it corresponded to two or more burts
            if len(np.unique(temp[index_nl])) > 2:
                # If yes, keep old labeling
                new_labeled_image[index_nl] = temp[index_nl]

        # Check if new labels contain old ones
        for nl in new_labels:
            # For a given label in the new labeled image
            intersection = []
            index_nl = np.where(new_labeled_image == nl)[0]
            # For a given label in the old labeled image
            for l in labels:
                # Keep the indexes where new-contains-old labels
                index_l = np.where(labeled_image == l)[0]
                if len(np.isin(index_l, index_nl)):
                    intersection.append(l)

            # Update region of intersect in case any was found
            if len(intersection) > 0:
                intersection = np.hstack(intersection)
                indexes = np.where(np.intersect1d(temp, intersection))
                temp[index_nl] = intersection.max()
            else:
                max_label = max_label + 1
                temp[index_nl] = max_label

        # Update labeled image
        labeled_image, labels, nlabels = new_labeled_image, new_labels, new_nlabels
        # Update threshold
        thr = thr - gamma

    # Reset labels in the final image
    labeled_image, labels, nlabels = return_labeled_image(
        labeled_image.reshape(size), 0
    )

    return labeled_image


z = W_theta[0]
z = (z - z.mean()) / z.std()
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.imshow(z, aspect="auto", origin="lower", cmap="turbo", vmax=5)
plt.colorbar()
plt.subplot(1, 2, 2)
plt.imshow(
    detect_burts(W_theta[0], 5, 1, 0.1) > 0,
    aspect="auto",
    origin="lower",
    cmap="binary_r",
)


z = W_gamma[0]
z = (z - z.mean()) / z.std()
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.imshow(z, aspect="auto", origin="lower", cmap="turbo", vmax=5)
plt.subplot(1, 2, 2)
plt.imshow(
    detect_burts(W_gamma[0], 6, 2, 0.1) > 0,
    aspect="auto",
    origin="lower",
    cmap="binary_r",
)


pos = 0
z = (W_gamma[pos] - W_gamma[pos].std()) / W_gamma[pos].std()


plt.imshow(W_gamma[pos], origin="lower", aspect="auto", cmap="turbo")
plt.title("spectogram")
plt.savefig("figures/frame1.png", dpi=300)


plt.imshow(
    z,
    origin="lower",
    aspect="auto",
    cmap="turbo",
)
plt.title("z-scored spectogram")
plt.savefig("figures/frame2.png", dpi=300)


frame = 3
for thr in [4, 3, 2, 1]:
    mask = detect_burts(W_gamma[pos], 5, thr, 0.1) > 0
    x = np.nan * (z * mask) + z
    plt.imshow(
        z * (np.where(mask, np.nan, mask) + 1),
        origin="lower",
        aspect="auto",
        cmap="turbo",
    )
    plt.title(f"threshold = {thr} STDs")
    plt.savefig(f"figures/frame{frame}.png", dpi=300)
    plt.close()
    frame = frame + 1
plt.tight_layout()





def extract_features(W, labeled_bursts):

    unique_labels = np.unique(labeled_bursts)[1:]
    nlabels = len(unique_labels)

    # Avg. and STD of each burst
    mean_amplitude = np.empty(nlabels)
    std_amplitude = np.empty(nlabels)

    # Avg., STD of the frequency of each burst and their weighted version
    mean_freq = np.empty(nlabels)
    std_freq = np.empty(nlabels)
    w_mean_freq = np.empty(nlabels)
    w_std_freq = np.empty(nlabels)
    peak_freq = np.empty(nlabels)

    # Avg., STD of the time of each burst and their weighted version
    mean_time = np.empty(nlabels)
    std_time = np.empty(nlabels)
    w_mean_time = np.empty(nlabels)
    w_std_time = np.empty(nlabels)
    t_start = np.empty(nlabels)
    t_stop = np.empty(nlabels)
    duration = np.empty(nlabels)

    labels_flattened = labeled_bursts.reshape(-1)
    W_stacked = W.stack(flat=("freqs", "times"))

    times = W_stacked.times.data
    freqs = W_stacked.freqs.data

    for pos, label in enumerate(unique_labels):
        mean_amplitude[pos] = W_stacked[labels_flattened == label].mean()
        std_amplitude[pos] = W_stacked[labels_flattened == label].std()

        mean_freq[pos] = freqs[labels_flattened == label].mean()
        std_freq[pos] = freqs[labels_flattened == label].std()
        w_mean_freq[pos], w_std_freq[pos] = weighted_avg_and_std(
            freqs[labels_flattened == label], W_stacked[labels_flattened == label]
        )
        peak_freq[pos] = freqs[labels_flattened == label].max()

        mean_time[pos] = times[labels_flattened == label].mean()
        std_time[pos] = times[labels_flattened == label].std()
        w_mean_time[pos], w_std_time[pos] = weighted_avg_and_std(
            times[labels_flattened == label], W_stacked[labels_flattened == label]
        )

        t_start[pos] = times[labels_flattened == label].min()
        t_stop[pos] = times[labels_flattened == label].max()

        duration[pos] = t_stop[pos] - t_start[pos]

    return (
        mean_amplitude,
        std_amplitude,
        mean_freq,
        std_freq,
        w_mean_freq,
        w_std_freq,
        peak_freq,
        mean_time,
        std_time,
        w_mean_time,
        w_std_time,
        t_start,
        t_stop,
        duration,
    )


def _for_batch(W):
    init = int(((W.max("times") - W.mean("times")) / W.std("times")).max().data.item())
    return detect_burts(W, init, 1, 0.1)


# define the function to compute in parallel
parallel, p_fun = parallel_func(_for_batch, verbose=False, n_jobs=20, total=100)

labeled_theta_bursts = parallel(p_fun(W) for W in W_theta[:100])


labeled_theta_bursts = np.stack(labeled_theta_bursts)


def weighted_avg_and_std(values, weights):
    """
    Return the weighted average and standard deviation.

    values, weights -- NumPy ndarrays with the same shape.
    """
    average = np.average(values, weights=weights)
    # Fast and numerically precise:
    variance = np.average((values - average) ** 2, weights=weights)
    return (average, np.sqrt(variance))


feature_names = [
    "mean_amplitude",
    "std_amplitude",
    "mean_freq",
    "std_freq",
    "w_mean_freq",
    "w_std_freq",
    "peak_freq",
    "mean_time",
    "std_time",
    "w_mean_time",
    "w_std_time",
    "t_start",
    "t_stop",
    "duration",
]

features = {}

for name in feature_names:

    features[name] = []

for i in tqdm(range(100)):
    out = extract_features(W_theta[i], labeled_theta_bursts[i])
    for pos, name in enumerate(feature_names):
        features[name] += [out[pos]]


plt.figure(figsize=(12, 8))
pos = 1
for name in feature_names:
    if name not in ["t_start", "t_stop"]:
        plt.subplot(3, 4, pos)
        plt.hist(np.hstack(features[name]), 50)
        plt.title(name)
        pos = pos + 1
plt.tight_layout()


def _for_batch(W):
    init = int(((W.max("times") - W.mean("times")) / W.std("times")).max().data.item())
    return detect_burts(W, init, 1, 0.1)


# define the function to compute in parallel
parallel, p_fun = parallel_func(_for_batch, verbose=False, n_jobs=20, total=100)

labeled_gamma_bursts = parallel(p_fun(W) for W in W_gamma[:100])


labeled_gamma_bursts = np.stack(labeled_gamma_bursts)


features_gamma = {}

for name in feature_names:

    features_gamma[name] = []

for i in tqdm(range(100)):
    out = extract_features(W_gamma[i], labeled_gamma_bursts[i])
    for pos, name in enumerate(feature_names):
        features_gamma[name] += [out[pos]]


plt.figure(figsize=(12, 8))
pos = 1
for name in feature_names:
    if name not in ["t_start", "t_stop"]:
        plt.subplot(3, 4, pos)
        plt.hist(np.hstack(features_gamma[name]), 50)
        plt.title(name)
        pos = pos + 1
plt.tight_layout()


import matplotlib as mpl

plt.figure(figsize=(9, 3.5))

plt.subplot(121)

plt.hist2d(
    np.hstack(features["mean_freq"]),
    np.log(np.hstack(features["mean_amplitude"])),
    bins=(20, 20),
    norm=mpl.colors.LogNorm(),
    cmap="turbo",
)
plt.ylabel("log(Amplitude)")
plt.xlabel("frequency [Hz]")
plt.title(r"$\theta$-bursts")
plt.colorbar()

plt.subplot(122)

plt.hist2d(
    np.hstack(features_gamma["mean_freq"]),
    np.log(np.hstack(features_gamma["mean_amplitude"])),
    bins=(20, 20),
    norm=mpl.colors.LogNorm(),
    cmap="turbo",
)
plt.ylabel("log(Amplitude)")
plt.xlabel("frequency [Hz]")
plt.title(r"$\gamma$-bursts")
plt.colorbar()

plt.tight_layout()


import numba as nb


@nb.njit
def overlaps(theta_timings: list, gamma_timings: list):
    n_theta = len(theta_timings)
    n_gamma = len(gamma_timings)

    n_overlaps = np.empty(n_gamma, dtype=np.int8)

    for i in range(n_gamma):
        temp = np.logical_and(
            theta_timings[:, 0] - gamma_timings[i, 0] < 0,
            theta_timings[:, 1] - gamma_timings[i, 0] > 0,
        )

        temp = np.logical_and(
            temp,
            np.logical_and(
                theta_timings[:, 0] - gamma_timings[i, 1] < 0,
                theta_timings[:, 1] - gamma_timings[i, 1] > 0,
            ),
        )

        n_overlaps[i] = temp.sum()

    return n_overlaps


n_overlaps = []

for i in range(100):

    T_theta = np.stack((features["t_start"][i], features["t_stop"][i]), axis=1)
    T_gamma = np.stack(
        (features_gamma["t_start"][i], features_gamma["t_stop"][i]), axis=1
    )

    n_overlaps += [overlaps(T_theta, T_gamma)]


n_overlaps = np.hstack(n_overlaps)
amplitudes = np.hstack(features_gamma["mean_amplitude"])


plt.figure(figsize=(9, 3.5))

ax = plt.subplot(121)
n, x = np.histogram(n_overlaps, bins=[0, 1, 2, 3, 4])
plt.bar(x[:-1], n / n.sum())
plt.xlabel("#theta bursts per gamma burst")
[ax.spines[key].set_visible(False) for key in ["top", "right"]]
ax = plt.subplot(122)
import pandas as pd
import seaborn as sns

df = pd.DataFrame(
    np.stack((n_overlaps, amplitudes), axis=1), columns=["overlap", "amplitude"]
)
sns.boxplot(data=df, x="overlap", y="amplitude", showfliers=False, color="lightblue")
[ax.spines[key].set_visible(False) for key in ["top", "right"]]

plt.xticks(rotation=90)

plt.tight_layout()


SIZES = []
FREQS = []

for pos in tqdm(range(100)):

    times = W_gamma[pos].stack(flat=("freqs", "times")).times.data
    freqs = W_gamma[pos].stack(flat=("freqs", "times")).freqs.data

    labeled_bursts = labeled_gamma_bursts[pos].reshape(-1)

    unique_labels = np.unique(labeled_bursts)[1:]
    n_labels = len(unique_labels)

    S = np.zeros((n_labels, n_labels))
    F_min = np.zeros(n_labels)
    F_max = np.zeros(n_labels)
    F_mean = np.zeros(n_labels)

    for l_i in unique_labels - 1:
        F_min[l_i] = freqs[labeled_bursts == l_i + 1].min()
        F_max[l_i] = freqs[labeled_bursts == l_i + 1].max()
        F_mean[l_i] = freqs[labeled_bursts == l_i + 1].mean()

    for l_i in range(n_labels - 1):
        times_i = times[labeled_bursts == l_i + 1]
        s_i = len(times_i)
        for l_j in range(l_i, n_labels):
            if F_max[l_i] < F_min[l_j]:
                times_j = times[labeled_bursts == l_j + 1]
                s_j = len(times_j)
                S[l_i, l_j] = len(np.intersect1d(times_i, times_j)) / np.min((s_i, s_j))
    np.fill_diagonal(S, 0)
    SIZES += [S]
    FREQS += [F_mean]


x, y = [], []
for i in range(len(SIZES)):
    pi, pj = np.where(SIZES[i] > 0.95)
    x += [FREQS[i][pi]]
    y += [FREQS[i][pj]]

x = np.hstack(x)
y = np.hstack(y)
plt.hist2d(x, y, cmap="hot_r")
plt.xlabel("Frequency [Hz]")
plt.ylabel("Frequency [Hz]")
plt.title(r"Density of harmonic $\gamma$ bursts")
plt.colorbar()


SIZES = []
FREQS = []

for pos in tqdm(range(100)):

    times = W_theta[pos].stack(flat=("freqs", "times")).times.data
    freqs = W_theta[pos].stack(flat=("freqs", "times")).freqs.data

    labeled_bursts = labeled_theta_bursts[pos].reshape(-1)

    unique_labels = np.unique(labeled_bursts)[1:]
    n_labels = len(unique_labels)

    S = np.zeros((n_labels, n_labels))
    F_min = np.zeros(n_labels)
    F_max = np.zeros(n_labels)
    F_mean = np.zeros(n_labels)

    for l_i in unique_labels - 1:
        F_min[l_i] = freqs[labeled_bursts == l_i + 1].min()
        F_max[l_i] = freqs[labeled_bursts == l_i + 1].max()
        F_mean[l_i] = freqs[labeled_bursts == l_i + 1].mean()

    for l_i in range(n_labels - 1):
        times_i = times[labeled_bursts == l_i + 1]
        s_i = len(times_i)
        for l_j in range(l_i, n_labels):
            if F_max[l_i] < F_min[l_j]:
                times_j = times[labeled_bursts == l_j + 1]
                s_j = len(times_j)
                S[l_i, l_j] = len(np.intersect1d(times_i, times_j)) / np.min((s_i, s_j))
    np.fill_diagonal(S, 0)
    SIZES += [S]
    FREQS += [F_mean]


x, y = [], []
for i in range(len(SIZES)):
    pi, pj = np.where(SIZES[i] > 0.95)
    x += [FREQS[i][pi]]
    y += [FREQS[i][pj]]

x = np.hstack(x)
y = np.hstack(y)
plt.hist2d(x, y, cmap="hot_r", density=True)
plt.xlabel("Frequency [Hz]")
plt.ylabel("Frequency [Hz]")
plt.title(r"Density of harmonic $\theta$ bursts")
plt.colorbar()


np.where(n_overlaps[:36] == 2)


plt.imshow(labeled_gamma_bursts[0] == 16, aspect="auto", cmap="binary_r")


composite.isel(bands=1, times=slice(6539, 6612), batches=0).plot()
composite.isel(bands=0, times=slice(6539, 6612), batches=0).plot()


W_gamma.times.data[(labeled_gamma_bursts[0] == 26).sum(0) > 0]



